[{"content":"A comprehensive Go microservice that fetches, consolidates, and analyzes hotel data from multiple sources.\nFeatures multi-source data aggregation, Retrieval Augmented Generation based recommendation engine with cutting edge LLMs.\nStatus: In active development.\nURL: Not yet public.\nArchitecture Alpaca is a microservice and utility suite that:\nFetches hotel data from multiple sources (Amadeus, Expedia, Tripadvisor, Google, Booking.com) Consolidates hotel data into a unified schema Crawls reviews from multiple sources (Tripadvisor, Google, Expedia, Booking, hotel websites, etc.) Vectorizes reviews and hotel data into RAG capable vector database. Uses LLM (GPT-4, Claude, Grok) to analyze reviews for Quality and Quiet Generates intelligent recommendations based on review analysis Stores data in SQLite (default) with raw SQL Uses a generalized provider interface for easy API integration Processes data in concurrent batches with rate limiting Github Repo\nCurrent Project Structure Note this is a development stage service and will likely undergo changes.\nalpaca/ ‚îú‚îÄ‚îÄ alpaca/ ‚îÇ ‚îú‚îÄ‚îÄ main.go # Main entry point - hotel data worker ‚îÇ ‚îú‚îÄ‚îÄ generate_cities.go # City data generation utility (reference) ‚îÇ ‚îú‚îÄ‚îÄ generated_top_cities.go # Generated top cities data (reference) ‚îÇ ‚îú‚îÄ‚îÄ REVIEW_PROCESSING.md # Review processing documentation ‚îÇ ‚îú‚îÄ‚îÄ models/ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ hotel.go # Original Amadeus hotel models ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ hotel_extended.go # Extended hotel models with recommendations ‚îÇ ‚îú‚îÄ‚îÄ services/ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ hotel_service.go # Hotel business logic (Amadeus) ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ hotel_service_extended.go # Extended hotel service (multi-source) ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ review_crawler.go # Review crawling from multiple sources ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ llm_service.go # LLM integration (GPT-4, Claude, Grok) ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ recommendation_service.go # Recommendation orchestration ‚îÇ ‚îú‚îÄ‚îÄ database/ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ database.go # SQLite database connection and schema ‚îÇ ‚îî‚îÄ‚îÄ utils/ ‚îÇ ‚îî‚îÄ‚îÄ constants.go # Constants and test data ‚îú‚îÄ‚îÄ go.mod # Go module definition ‚îú‚îÄ‚îÄ Dockerfile # Docker build configuration ‚îî‚îÄ‚îÄ README.md # This file Features ‚úÖ Simplified Architecture Single Microservice: One focused service for hotel data collection Raw SQL: No ORM overhead, direct SQL control, ready for Redshift/Postgres SQLite First: Simple, file-based database (easy to migrate to Postgres/Redshift later) Generalized API Interface: Easy to add new hotel data providers ‚úÖ Multi-Source Hotel Data Collection Amadeus API: Hotel list, search, and ratings data Expedia: Hotel listings and reviews (interface ready) Tripadvisor: Hotel data and reviews (interface ready) Google Places: Hotel data and reviews (interface ready) Booking.com: Hotel data and reviews (interface ready) Consolidated Schema: Unified hotel table with ratings from all sources ‚úÖ Review Processing \u0026amp; LLM Analysis Multi-Source Review Crawling: Automatically fetches reviews from: Tripadvisor, Google, Expedia, Booking.com Hotel websites, Bing, Yelp Vector database will be used for fine-tuned models in iteration 2 LLM-Powered Analysis: Uses GPT-4, Claude, or Grok to analyze areviews Quality Detection: Identifies hotels with excellent service, cleanliness, amenities Quiet Detection: Identifies quiet, peaceful hotels away from noise Intelligent Recommendations: Combines quality and quiet analysis for recommendations Admin Override: Admin flag to enable/disable hotels regardless of analysis ‚úÖ Advanced Processing Proper Pagination: Handles multi-page API responses automatically Concurrent Processing: Uses goroutines for parallel data fetching Rate Limiting: Respects API limits with configurable delays Error Handling: Graceful degradation and detailed error logging Invalid ID Tracking: Skips hotel IDs that consistently fail üöÄ Getting Started Prerequisites Go 1.23+ Amadeus API credentials (test or production) Environment Variables Create a .env file in the project root:\n# Amadeus API Credentials AMD=your_client_id AMS=your_client_secret # Optional: Override default API URLs AMADEUS_HOTEL_LIST_URL=https://test.api.amadeus.com/v1/reference-data/locations/hotels/by-city AMADEUS_HOTEL_SEARCH_URL=https://test.api.amadeus.com/v2/shopping/hotel-offers AMADEUS_HOTEL_RATINGS_URL=https://test.api.amadeus.com/v2/e-reputation/hotel-sentiments # Optional: Database path (defaults to ./alpaca.db) SQLITE_DB_PATH=./alpaca.db # Optional: Search radius configuration HOTEL_SEARCH_RADIUS=100 HOTEL_SEARCH_RADIUS_UNIT=MILE Running the Service go build -o alpaca ./alpaca/alpaca ./alpaca Or from the alpaca/alpaca directory:\ncd alpaca/alpaca go build -o alpaca ./alpaca The service will:\nConnect to SQLite database (creates if doesn\u0026rsquo;t exist) Fetch hotel list for Austin, TX (default city) Fetch detailed search data for all hotels Fetch ratings data for test hotel IDs Database Schema The service uses a simple normalized schema with three main tables:\n-- Basic hotel information CREATE TABLE hotels ( id INTEGER PRIMARY KEY AUTOINCREMENT, hotel_id TEXT UNIQUE NOT NULL, type TEXT, chain_code TEXT, dupe_id INTEGER, name TEXT, iata_code TEXT, address TEXT, -- JSON stored as TEXT geo_code TEXT, -- JSON stored as TEXT distance TEXT, -- JSON stored as TEXT last_update TEXT, created_at DATETIME DEFAULT CURRENT_TIMESTAMP, updated_at DATETIME DEFAULT CURRENT_TIMESTAMP ); -- Detailed hotel metadata CREATE TABLE hotel_search_data ( id INTEGER PRIMARY KEY AUTOINCREMENT, hotel_id TEXT UNIQUE NOT NULL, type TEXT, chain_code TEXT, dupe_id INTEGER, name TEXT, rating INTEGER, official_rating INTEGER, description TEXT, -- JSON stored as TEXT media TEXT, -- JSON stored as TEXT amenities TEXT, -- JSON stored as TEXT address TEXT, -- JSON stored as TEXT contact TEXT, -- JSON stored as TEXT policies TEXT, -- JSON stored as TEXT available INTEGER DEFAULT 0, offers TEXT, -- JSON stored as TEXT self TEXT, hotel_distance TEXT, -- JSON stored as TEXT last_update TEXT, created_at DATETIME DEFAULT CURRENT_TIMESTAMP, updated_at DATETIME DEFAULT CURRENT_TIMESTAMP, FOREIGN KEY (hotel_id) REFERENCES hotels(hotel_id) ); -- Guest ratings and sentiment CREATE TABLE hotel_ratings_data ( id INTEGER PRIMARY KEY AUTOINCREMENT, hotel_id TEXT UNIQUE NOT NULL, type TEXT, number_of_reviews INTEGER, number_of_ratings INTEGER, overall_rating INTEGER, sentiments TEXT, -- JSON stored as TEXT last_update TEXT, created_at DATETIME DEFAULT CURRENT_TIMESTAMP, updated_at DATETIME DEFAULT CURRENT_TIMESTAMP, FOREIGN KEY (hotel_id) REFERENCES hotels(hotel_id) ); -- Track invalid hotel IDs to skip in future runs CREATE TABLE invalid_hotel_search_ids ( id INTEGER PRIMARY KEY AUTOINCREMENT, hotel_id TEXT UNIQUE NOT NULL, created_at DATETIME DEFAULT CURRENT_TIMESTAMP ); API Provider Interface The service uses a generalized HotelAPIProvider interface, making it easy to add new hotel data sources:\ntype HotelAPIProvider interface { GetOAuthToken(ctx context.Context) (string, error) FetchHotelsList(ctx context.Context, cityCode string, token string) ([]models.HotelAPIItem, string, error) FetchHotelSearchData(ctx context.Context, hotelID string, token string) (*models.HotelSearchData, error) FetchHotelRatingsData(ctx context.Context, hotelID string, token string) (*models.HotelRatingsData, error) } Currently implemented:\nAmadeusProvider: Full Amadeus API integration Future providers can be added by implementing this interface.\nData Flow OAuth Token: Service authenticates with Amadeus API Hotel List: Fetches basic hotel data by city (with pagination) Hotel IDs: Extracts all hotel IDs for detailed processing Search Data: Concurrently fetches detailed hotel metadata (5 concurrent requests) Ratings Data: Concurrently fetches ratings and sentiment data (1 concurrent request for rate limiting) Performance Features Concurrent Processing: 5x faster data fetching with goroutines Rate Limiting: API-friendly request patterns (100-200ms delays) Pagination Handling: Efficient memory usage for large datasets Database Indexing: Optimized query performance Error Recovery: Graceful handling of API failures Invalid ID Tracking: Skips problematic hotel IDs automatically Review Processing See REVIEW_PROCESSING.md for detailed documentation on:\nReview crawling from multiple sources LLM analysis for Quality and Quiet detection Recommendation generation Usage examples Quick Start - Review Processing // Initialize services db, _ := database.NewDatabase() hotelService := services.NewHotelService(db) reviewCrawler := services.NewReviewCrawlerService(db) llmProvider := services.NewOpenAIProvider(os.Getenv(\u0026#34;OPENAI_API_KEY\u0026#34;)) llmService := services.NewLLMService(llmProvider) recommendationService := services.NewRecommendationService( hotelService, reviewCrawler, llmService, ) // Process recommendations for a hotel err := recommendationService.ProcessHotelRecommendations(ctx, \u0026#34;hotel-id\u0026#34;) Takeaways Database Backend Options SQLite (Current): Good for development and small datasets\nPros: Simple, no server needed, fast for reads Cons: Limited concurrency, not ideal for high write loads PostgreSQL: Recommended for production\nPros: Better concurrency, JSON support, full SQL features Cons: Requires server setup AWS Redshift: For analytics workloads\nPros: Columnar storage, optimized for analytics Cons: More complex setup, better for read-heavy analytics once data is standardized MongoDB: If we need future document flexibility and secondary storage\nPros: Native JSON, flexible schema Cons: Different query model, may need to rethink relationships Recommendation: Start with SQLite for development, migrate to PostgreSQL for production. The raw SQL approach makes migration straightforward.\nCode Simplification ToDo Struct Simplification:\nConsider flattening some nested JSON structures Remove unused fields from API responses Create separate structs for database vs API models Database Code:\nAdd connection pooling configuration Implement prepared statements for better performance Add transaction support for batch operations Error Handling:\nCreate custom error types for better error handling Add retry logic with exponential backoff Implement circuit breaker pattern for API calls Configuration:\nMove hardcoded values to config file Implement feature flags Add validation for environment variables Support multiple city codes with Geolocation Testing:\nAdd unit tests for database operations Add integration tests for API provider, LLM and Vectors Mock external API calls for testing Development Building go build -o alpaca ./alpaca/alpaca Running Tests go test ./... Code Style The project follows standard Go conventions:\nUse gofmt for formatting Use golint for linting Follow Go naming conventions License MIT\n","permalink":"https://ericarellano.tech/posts/go-hotel-llm/","summary":"\u003cp\u003eA comprehensive Go microservice that fetches, consolidates, and analyzes hotel data from multiple sources.\u003c/p\u003e\n\u003cp\u003eFeatures multi-source data aggregation, Retrieval Augmented Generation based recommendation engine with cutting edge LLMs.\u003c/p\u003e\n\u003cp\u003eStatus: In active development.\u003c/p\u003e\n\u003cp\u003eURL: Not yet public.\u003c/p\u003e\n\u003ch2 id=\"architecture\"\u003eArchitecture\u003c/h2\u003e\n\u003cp\u003eAlpaca is a microservice and utility suite that:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFetches hotel data from multiple sources (Amadeus, Expedia, Tripadvisor, Google, Booking.com)\u003c/li\u003e\n\u003cli\u003eConsolidates hotel data into a unified schema\u003c/li\u003e\n\u003cli\u003eCrawls reviews from multiple sources (Tripadvisor, Google, Expedia, Booking, hotel websites, etc.)\u003c/li\u003e\n\u003cli\u003eVectorizes reviews and hotel data into RAG capable vector database.\u003c/li\u003e\n\u003cli\u003eUses LLM (GPT-4, Claude, Grok) to analyze reviews for Quality and Quiet\u003c/li\u003e\n\u003cli\u003eGenerates intelligent recommendations based on review analysis\u003c/li\u003e\n\u003cli\u003eStores data in SQLite (default) with raw SQL\u003c/li\u003e\n\u003cli\u003eUses a generalized provider interface for easy API integration\u003c/li\u003e\n\u003cli\u003eProcesses data in concurrent batches with rate limiting\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/chukiagosoftware/alpaca\"\u003eGithub Repo\u003c/a\u003e\u003c/p\u003e","title":"Alpaca - Hotel Quality Recommendations"},{"content":"About Me Officially a Computer and Electrical Engineer, B.Sc. (nice!). I\u0026rsquo;ve mostly been a Network and Security, then Devops, Site Reliability and more recently an AI, AWS/GCP and Go engineer. This is my blog, not really a portfolio and not too serious.\nJust some notes and musings on my journey as a software engineer. My passion is building fun applications, staying at the cutting edge of Cloud, Kubernetes, OpenTelemetry and AI of course. I\u0026rsquo;ve built and helped to scale and observe a variety of SaaS, Webex Videoconferencing, Jitsi videoconference, SIP/WebRTC VoIP and IP Telephony Contact Center, Big Data parallel processing, Machine Learning neural network models, Large Language Generative Pretrained Transformer Models, Confluent Kafka event streaming, Postgres/MySQL/Redshift/Aurora database modeling, data ingestion, processing, LLM model fine-tuning.\nRecently I\u0026rsquo;ve worked on vectorizing data with Go for RAG, modeling input data with AWS Glue, Athena and Redshift implementing performant AI-based systems in AWS Kubernetes with OpenTelemetry, Prometheus and/or Datadog for real time metrics and analytics.\nMy most recent CI/CD and programming language are Github Actions and Go. I\u0026rsquo;ve also worked heavily with Jenkins (I know), ArgoCD, Google Cloudbuild, Python, Javascript, Ansible but, I\u0026rsquo;m not going to talk about those.\nMy speciality du jour is building, deploying apps written in Go and Python and handling AWS, Kubernetes, OpenTelemetry or Prometheus metrics for an end to end system such as for Generative AI (Large Language Models) applications.\nI\u0026rsquo;m somewhat of a nerd and I am really good with Terraform.\nLike good? Maybe.\nWhile I am solely responsible for all this crap- I mean, content- I do have to thank Grok and Claude for helping make the conversion fast and mostly painless. Also, thanks to my friends and family for supporting me and my wife.\nThe Icons The site runs on Python 3.10 with the Pelican 4.8.0 microblog software and my custom Python Markdown 3.4.1 extension for the Github Emojis. These are just the publicly available emojis which you can see around the pages. I find that there are some really, really cool ones and thus without further ado.\nActually, this is no longer true. This is now a Go Hugo site.\nHugo is really cool, has Go templating engine, emojis and others are a breeze to add using shortcodes and will allow me to delve more into the Go ecosystem as I develop my Hotel Recommendations Rag application.\nThe Platform The blog code is deployed based on Git ops.\nIn other words, the Python code which generates html, is pushed to a main branch integrated with Ci/Cd service Netlify\nI use Jetbrains Pycharm, so this is done either by typing git push in the Terminal or Cmd-K in the IDE window. I highly recommend it.\nOther options I considered for deployment over the years were Google App Engine, Render and a VM on Digital Ocean or a free Oracle VM.\nAfter running off a Google Bucket with local-only Pelican html generation, I settled on Netlify a few years back because it works very nicely, can scale to a full-blown app with CDN and content managment and more importantly Netlify had Python 3.10 compatible images at the time.\nThis is a static HTML with some Javascript and a Pelican template. So, there is only one contributor, no merging, PRs and other differences with a production distributed microservice or hybrid service architecture. Netlify has a nice build image for Python and static sites.\nAgain, the above is no longer true. This is now a Hugo static microblog which is published via Github Actions to Github Pages.\nThe conversion from Python (Pelican) to Go (Hugo) was done with the help of Grok to provide some handy Python scripts. Github Pages is now very mature and Github Actions allows me to build the suite upon pushes to Main, then deploy off a special branch all without leaving Github.\nThe Future I suppose I plan to add more content, enable searching and make the text and appearance more professional looking.\nAlso, once completed we will have a widget with my Hotel Recommendation assistant.\nI also plan to migrate to Hugo or another static site generator.\nAs noted a bunch of times (sorry) this migration is now done. This is now a Go Hugo site. Did I mention that?\nThe Links Here are some useful links:\nMy Data Engineering Professional Cert My Cisco Gen AI Blue Belt My Github My LinkedIn News Kubernetes Go Terraform ","permalink":"https://ericarellano.tech/about/","summary":"\u003ch2 id=\"about-me-hahahugoshortcode4s0hbhb\"\u003eAbout Me \u003cimg src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f9d9-2642.png?v8\" alt=\":mage_man:\" class=\"emoji\" style=\"display: inline-block; height: 1em; width: 1em; vertical-align: middle;\" /\u003e\u003c/h2\u003e\n\u003cp\u003eOfficially a Computer and Electrical Engineer, B.Sc. (nice!). I\u0026rsquo;ve mostly been a Network and Security, then Devops,\nSite Reliability and more recently an AI, AWS/GCP and Go engineer. This is my blog, not really a portfolio and not too serious.\u003c/p\u003e\n\u003cp\u003eJust some notes and musings on my journey as a software engineer. \u003cimg src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f9cb.png?v8\" alt=\":bubble_tea:\" class=\"emoji\" style=\"display: inline-block; height: 1em; width: 1em; vertical-align: middle;\" /\u003e\u003c/p\u003e\n\u003cp\u003eMy passion is building fun applications, staying at the cutting edge of Cloud, Kubernetes,\nOpenTelemetry and AI of course. \u003cimg src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f434.png?v8\" alt=\":horse:\" class=\"emoji\" style=\"display: inline-block; height: 1em; width: 1em; vertical-align: middle;\" /\u003e\u003c/p\u003e\n\u003cimg src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png?v8\" alt=\":computer:\" class=\"emoji\" style=\"display: inline-block; height: 1em; width: 1em; vertical-align: middle;\" /\u003e\n\u003cp\u003eI\u0026rsquo;ve built and helped to scale and observe a variety of SaaS, Webex Videoconferencing, Jitsi videoconference,\nSIP/WebRTC VoIP and IP Telephony Contact Center, Big Data parallel processing, Machine Learning neural network models,\nLarge Language Generative Pretrained Transformer Models, Confluent Kafka event streaming, Postgres/MySQL/Redshift/Aurora database\nmodeling, data ingestion, processing, LLM model fine-tuning.\u003c/p\u003e\n\u003cp\u003eRecently I\u0026rsquo;ve worked on vectorizing data with Go for RAG, modeling input data with AWS Glue, Athena and Redshift\nimplementing performant AI-based systems in AWS Kubernetes with\nOpenTelemetry, Prometheus and/or Datadog for real time metrics and analytics.\u003c/p\u003e\n\u003cp\u003eMy most recent CI/CD and programming language are Github Actions and Go. I\u0026rsquo;ve also worked heavily with Jenkins (I know), ArgoCD, Google Cloudbuild,\nPython, Javascript, Ansible but, I\u0026rsquo;m not going to talk about those.\u003c/p\u003e\n\u003cp\u003eMy speciality du jour is building, deploying apps written in Go and Python and handling AWS, Kubernetes, OpenTelemetry or Prometheus metrics\nfor an end to end system such as for Generative AI (Large Language Models) applications.\u003c/p\u003e\n\u003cp\u003eI\u0026rsquo;m somewhat of a nerd and I am \u003cimg src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f913.png?v8\" alt=\":nerd_face:\" class=\"emoji\" style=\"display: inline-block; height: 1em; width: 1em; vertical-align: middle;\" /\u003e really good with Terraform.\u003c/p\u003e\n\u003cp\u003eLike \u003cimg src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f99b.png?v8\" alt=\":hippopotamus:\" class=\"emoji\" style=\"display: inline-block; height: 1em; width: 1em; vertical-align: middle;\" /\u003e good? Maybe.\u003c/p\u003e\n\u003cp\u003eWhile I am solely responsible for all this crap- I mean, content- I do have to thank Grok and Claude for helping make the\nconversion fast and mostly painless. Also, thanks to my friends and family for supporting me and my wife.\u003c/p\u003e","title":"Intro"},{"content":"Building a Modern Application on AWS EKS with Pulumi: A Practical Journey The application leverages PostgreSQL for data persistence, Kafka for event streaming, and a mix of Go, Python, and JavaScript components, all deployed on AWS Elastic Kubernetes Service (EKS) using Pulumi in Go. The cluster runs in private subnets with VPC endpoints for a fully isolated setup.\nThe Go Code: Precision in Pulumi The backbone of our infrastructure is a meticulously structured Go program using the Pulumi AWS and EKS SDKs (github.com/pulumi/pulumi-aws/sdk/v6/go/aws and github.com/pulumi/pulumi-eks/sdk/v1/go/eks). We defined a VPC with private subnets using aws.ec2.Vpc and aws.ec2.Subnet, ensuring each subnet has a dedicated route table created via aws.ec2.RouteTable and associated with aws.ec2.RouteTableAssociation. This setup isolates traffic, with privateSubnets passed as a pulumi.StringArrayOutput to the EKS cluster for node placement.\nThe EKS cluster itself is instantiated with eks.NewCluster, taking arguments like VpcId, SubnetIds, and InstanceType (e.g., t3.medium) to configure a default node group. A critical piece is the InstanceRole, an aws.iam.Role with a trust policy for ec2.amazonaws.com, attached to policies like AmazonEKSWorkerNodePolicy, AmazonEKSVPCResourceController, and AmazonEKS_CNI_Policy via iam.NewRolePolicyAttachment. This role, assigned to nodes, ensures proper cluster integration and CNI functionality. We set EndpointPrivateAccess: true and EndpointPublicAccess: true for flexibility, though nodes rely on private VPC endpoints.\nTo support private networking, we added VPC endpoints using aws.ec2.VpcEndpoint. The EKS endpoint (com.amazonaws.us-east-1.eks) and ECR endpoints (com.amazonaws.us-east-1.ecr.dkr and .ecr.api) are Interface types with PrivateDnsEnabled: true, tied to privateSubnets and the cluster‚Äôs node security group (cluster.NodeSecurityGroupIds). The S3 Gateway endpoint (com.amazonaws.us-east-1.s3) uses RouteTableIds derived dynamically from privateSubnets with an ApplyT function calling ec2.LookupRouteTable, corrected to target arn:aws:s3:::prod-us-east-1-starport-layer-bucket/* after fixing a ctx.Stack() bug. The final touch exports the kubeconfig (ctx.Export(\u0026quot;kubeconfig\u0026quot;, cluster.Kubeconfig)), enabling kubectl access.\nGithub Repo\nChallenge 1: Bootstrapping AL2023 Nodes Problem: Nodes wouldn‚Äôt register; kubelet failed with ‚ÄúFailed to load environment files‚Äù errors, and bootstrap.sh was missing.\nSolution: The AL2023 EKS AMI uses nodeadm. A security group blocked HTTPS to the EKS VPC endpoint (com.amazonaws.us-east-1.eks). We fixed the security group, confirmed connectivity with curl, and ran /opt/eks/nodeadm join --config /etc/eks/nodeadm/config.yaml. In Pulumi, we set InstanceRole in eks.ClusterArgs.\nChallenge 2: IAM Role Confusion Problem: Nodes hit ‚ÄúUnauthorized‚Äù errors due to a misconfigured IAM role.\nSolution: We used ServiceRole instead of InstanceRole. Switching to nodeRole with AmazonEKSWorkerNodePolicy, AmazonEKSVPCResourceController, and AmazonEKS_CNI_Policy fixed it. We mapped it in aws-auth ConfigMap (system:nodes) and added an eks.AccessEntry for API_AND_CONFIG_MAP mode, then ran pulumi up.\nChallenge 3: NetworkPluginNotReady Problem: Nodes stayed ‚ÄúNot Ready‚Äù with NetworkReady=false reason:NetworkPluginNotReady.\nSolution: The aws-node pod couldn‚Äôt pull its image from ECR without a VPC endpoint for com.amazonaws.us-east-1.ecr.dkr. We added the endpoint with PrivateDnsEnabled: true, restarted containerd and kubelet, and deleted the pod to retry.\nChallenge 4: S3 Access Denied Problem: A 403 Forbidden error occurred when fetching CNI image blobs from S3 (prod-us-east-1-starport-layer-bucket), despite ECR working.\nSolution: We added an S3 Gateway endpoint (com.amazonaws.us-east-1.s3) with a policy for arn:aws:s3:::prod-us-east-1-starport-layer-bucket/*. A Pulumi bug‚Äîctx.Stack() returning prod instead of us-east-1‚Äîmisnamed the bucket as prod-prod-. Fixing it to us-east-1 aligned the policy, resolving the 403.\nChallenge 5: Containerd vs. Docker Problem: Debugging assumed Docker, but AL2023 uses containerd‚Äîno docker command.\nSolution: We used ctr for testing (sudo ctr image pull ...) and confirmed containerd‚Äôs config (/etc/containerd/config.toml) relied on IMDS for ECR creds.\nThe Outcome After pulumi up, nodes registered and flipped to ‚ÄúReady‚Äù (kubectl get nodes), and aws-node pods ran (kubectl get pods -n kube-system), initializing the CNI.\nKey Takeaways AL2023 Shift: Use nodeadm and InstanceRole for node IAM. Private Networking: Add VPC endpoints (EKS, ECR, S3) and test with curl. IAM Precision: Verify roles and stack names. S3 Nuance: Fix ECR-to-S3 redirects with correct bucket policies. ","permalink":"https://ericarellano.tech/posts/eks-pulumi/","summary":"\u003ch1 id=\"building-a-modern-application-on-aws-eks-with-pulumi-a-practical-journey\"\u003eBuilding a Modern Application on AWS EKS with Pulumi: A Practical Journey\u003c/h1\u003e\n\u003cp\u003eThe application leverages PostgreSQL for data persistence, Kafka for event streaming, and a mix of Go, Python, and JavaScript components, all deployed on AWS Elastic Kubernetes Service (EKS) using Pulumi in Go. The cluster runs in private subnets with VPC endpoints for a fully isolated setup.\u003c/p\u003e\n\u003ch2 id=\"the-go-code-precision-in-pulumi\"\u003eThe Go Code: Precision in Pulumi\u003c/h2\u003e\n\u003cp\u003eThe backbone of our infrastructure is a meticulously structured Go program using the Pulumi AWS and EKS SDKs (\u003ccode\u003egithub.com/pulumi/pulumi-aws/sdk/v6/go/aws\u003c/code\u003e and \u003ccode\u003egithub.com/pulumi/pulumi-eks/sdk/v1/go/eks\u003c/code\u003e). We defined a VPC with private subnets using \u003ccode\u003eaws.ec2.Vpc\u003c/code\u003e and \u003ccode\u003eaws.ec2.Subnet\u003c/code\u003e, ensuring each subnet has a dedicated route table created via \u003ccode\u003eaws.ec2.RouteTable\u003c/code\u003e and associated with \u003ccode\u003eaws.ec2.RouteTableAssociation\u003c/code\u003e. This setup isolates traffic, with \u003ccode\u003eprivateSubnets\u003c/code\u003e passed as a \u003ccode\u003epulumi.StringArrayOutput\u003c/code\u003e to the EKS cluster for node placement.\u003c/p\u003e","title":"Building a Modern Application on AWS EKS with Pulumi: A Practical Journey"},{"content":"DevOps de Noche Today we will deploy Google Kubernetes Engine, Terraform Cloud worspaces and variable sets, Cloud Build triggers, Cloud Deploy Delivery Pipeline providing Continuous Delivery for Grafana and the sample (Google) microservices app via Skaffold.\nUn repositorio y presentaci√≥n para DevOps Days La Paz 2022 https://github.com/edamsoft-sre/DevOpsDeNoche\nHow to use the Terraform resources google_compute_network\ngoogle_compute_subnetwork\ntfe_workspace\ntfe_variable-set\ncloud_build_trigger\ngoogle_clouddeploy_delivery_pipeline\ngoogle_container_cluster\ngoogle_container_node_pool\nPrivate Google Kubernetes clusters with access control.\nPrivate Google Cloud Build worker pool\nSkaffold via Cloud Build and via Cloud Deploy\nKeynote Video Configuraci√≥n completa de Google Cloud Platform y Google Kubernetes Engine incluye red Compute Network CICD con Cloud Build y Cloud Deploy apps de ejemplo Grafana (monitoreo) y Microservices (ejemplo Google Kubernetes) Instalaci√≥n Cuenta gratuita de Terraform Cloud y API Token\nCuenta Free Tier de Google Cloud Platform\nLlave JSON Key de Usuario de Google Cloud con estos permisos\nCloud Build Service Agent Cloud Deploy Admin Compute Network Admin Kubernetes Engine Admin Project IAM Admin Service Account Admin Service Account User Service Usage Admin Esta llave se coloca en Terraform Cloud como variable de entorno GOOGLE_CREDENTIALS\nHacer Git Fork del repositorio https://github.com/edamsoft-sre/DevOpsDeNoche\nActualizar vcs_repo en los recursos tfe_workspace con los datos del Fork en\n3_infraestructura_escalable/terraform-cloud/workspaces.tf\nConfiguraci√≥n de Terraform Cloud para Workspaces y Variables Compartidas En 3_infraestructura_escalable/terraform-cloud se encuentra la base de la Infraestructura como C√≥digo, los Workspaces y Variables base de Terraform Cloud. Primero se configura este Workspace de forma manual con las siguientes variables:\nentorno: nombre del entorno como Demo, Dev, Prod tfe_email: correo de admin Terraform Cloud project: proyecto de GCP tfe_org: nombre de Organizacion Terraform Cloud region: Region Google Cloud oauth_token: un token oauth de Github con permiso para leer el repositorio TFE_TOKEN: el User API Token de Terraform Cloud Con este Workspace inicial se definen otros cuatro Workspaces de Terraform Cloud utilizando el proveedor TFE de Hashicorp. De esta forma automatizamos la configuraci√≥n de Terraform Cloud mismo.\nterraform-cloud - la configuraci√≥n base que brinda los otros 4 Workspace, y variables compartidas definidas arriba var.region var.subnets var.entorno De esta forma no es necesario repetir las variables en cada Workspace y configuraci√≥n. Una vez creados los Workspace de trabajo, se a√±ade la llave JSON de Google Cloud omo variable de entorno GOOGLE_CREDENTIALS\nWorkspaces de Trabajo devops-days-servicios-google configuraci√≥n de servicios API Google Cloud Platform creaci√≥n de usuario-gke, usuario-cloudbuild y usuario-grafana para correr estos servicios configuraci√≥n de Roles IAM para estas cuentas de servicios devops-days-red-vpc Configuraci√≥n de una Red VPC (Google Compute Network), Subnetworks, Router y Cloud NAT Kubernetes Esto permite crear clusters Kubernetes / Private GKE con subnets ya definidas y con acceso a Internet.\ndevops-days-kubernetes-us-central1-a Para mayor seguridad se configuran GKE Private con nodos privados (sin IP publica). Por conveniencia se mantiene el accesso a Kubernetes master con IP p√∫blica. Se puede controlar el acceso con las variables var.acceso_publico y var.cidr_autorizadas devops-days-kubernetes-us-east1-a Se pueden a√±adir m√°s clusters GKE Kubernetes a√±adiendo m√°s Workspace, en la configuracion existen dos En Terraform Cloud se define la variable var.zone que define la Zona donde colocar el cluster La configuraci√≥n est√° basada en una Region Google Cloud y puede tener hasta 4 clusters en cada zona. CICD devops-days-cicd El √∫ltimo Workspace de Terraform Cloud est√° dedicado a los Pipelines de CICD Integraci√≥n Continua con Google Cloud Build Despliegue Continuo con Google Cloud Deploy (un Pipeline gratis en Free Tier, adicionales $15-mes) Apps de Ejemplo Grafana Se despliega Grafana de la siguiente forma:\ngcloud deploy releases create grafana \u0026ndash;project=days-devops \u0026ndash;delivery-pipeline=pipeline-demo \u0026ndash;region=us-central1 \u0026ndash;to-target=dev-central1-a-target\nGoogle Microservices Una aplicaci√≥n de tienda en linea ejemplo de Google Cloud Platform Se despliega de forma automatica a Kubernetes utilizando Cloud Build y Skaffold Se debe hacer Git Fork del repo https://github.com/edamsoft-sre/google-microservices-demo Tabla de Contenido del Repositorio\nCap√≠tulo Zero: Formato y pruebas de variables Terraform 1 Probando Variables\nCap√≠tulo Uno: Dise√±ando Organizacion Google en Terraform para Escalar a Trescientos Desarrolladores. 2 Arquitectura de la Organizaci√≥n DevOps\nCap√≠tulo Dos: Definir la Organizacion, Roles y Permisos en Terraform 3 Terraform Cloud, GCP Network, Kubernetes, IAM\nCap√≠tulo Tres: Kubernetes, Red VPC 3 Kubernetes 3 Red VPC\nCap√≠tulo Cuatro: CI/CD y Pipelines para Desarrollo y Publicacion Agiles 3 Pipelines\n","permalink":"https://ericarellano.tech/posts/devopsdays/","summary":"\u003ch1 id=\"devops-de-noche-hahahugoshortcode11s0hbhb\"\u003eDevOps de Noche \u003cimg src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f304.png?v8\" alt=\":sunrise_over_mountains:\" class=\"emoji\" style=\"display: inline-block; height: 1em; width: 1em; vertical-align: middle;\" /\u003e\u003c/h1\u003e\n\u003cp\u003eToday we will deploy Google Kubernetes Engine, Terraform Cloud worspaces and variable sets, Cloud Build triggers, Cloud\nDeploy Delivery Pipeline providing Continuous Delivery for Grafana and the sample (Google) microservices app via\nSkaffold.\u003c/p\u003e\n\u003ch2 id=\"hahahugoshortcode11s1hbhb-un-repositorio-y-presentaci√≥n-para-devops-days-la-paz-2022\"\u003e\u003cimg src=\"https://github.githubassets.com/images/icons/emoji/unicode/2601.png?v8\" alt=\":cloud:\" class=\"emoji\" style=\"display: inline-block; height: 1em; width: 1em; vertical-align: middle;\" /\u003e Un repositorio y presentaci√≥n para DevOps Days La Paz 2022\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"github.com/edamsoft-sre/DevOpsDeNoche\"\u003ehttps://github.com/edamsoft-sre/DevOpsDeNoche\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"how-to-use-the-terraform-resources\"\u003eHow to use the Terraform resources\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003egoogle_compute_network\u003c/em\u003e\u003c/p\u003e","title":"My DevOps Days La Paz presentation with Terraform Cloud, GKE, Skaffold"},{"content":"Part 1: Deploying FastAPI app to AWS Lambda with Docker image This will show how to build a Docker container that can be run by AWS Lambda from the Elastic Container Registry.\nThe steps to follow on AWS are currently manual - via the AWS Console and the ECR provided Docker commands to upload images.\nIn future release, we will automate this release cycle using CodeDeploy, or possibly another AWS service if that turns out to be a better fit (CodePipeline? CloudFormation?)\nDeploying FastAPI to AWS Lambda Lambda allows multiple methods to provide the software artifacts to be executed as a Lambda Function. For a Python FastAPI app, the primary choices are a zip file and a Docker container image.\nTo deploy via zip file, follow the excellent instructions from Simon Willison at asgi-mangum To summarize, this method uses the wrapper library Mangum to translate between AWS Lambda\u0026rsquo;s (event, context) and ASGI\u0026rsquo;s standard HTTP interface.\nAWS Lambda Runtime Interface helper Amazon provides the awslambdaric helper library, or runtime interface client, to allow a Docker container to act as Lambda event receiver. This library can be easily added to Dockerfile and will work with Python 3.10, although not yet released on official AWS images. For the full dockerfile logic please refer to the AWS docs on PyPi. These also provide a multi-stage build to save image space.\nFROM python:3.10 RUN apt-get update \u0026amp;\u0026amp; \\ apt-get install -y \\ g++ \\ make \\ cmake \\ unzip \\ libcurl4-openssl-dev RUN pip install \\ --target ${FUNCTION_DIR} \\ awslambdaric In layman\u0026rsquo;s terms a Docker container provides an executable command interface (or none for background services) via the CMD and ENTRYPOINT directives. Lambda Functions can also overwrite these if needed.\nThe CMD in Docker tells whomever is \u0026ldquo;calling\u0026rdquo; the Docker container what code will actually run and with which parameters. The caller can then modify these, for example to change a default path, timeout or other configuration value. Usually, in Kubernetes for example, this is defined during engineering and seldom modified later in the deployment cycle.\nKubernetes also allows the Docker command and entrypoint to be overridden, confusingly, with it\u0026rsquo;s own Command and Args specification While confusing, both sets of directives are useful to allow custom, zonal/regional or otherwise different deployments from the same docker image, specifying values from the local execution context or infrastructure, or even date.\nLambda Entrypoint and CMD For a Lambda function, the container should be instructed to hand control of it\u0026rsquo;s output to the runtime interface client as follows:\nENTRYPOINT [ \u0026quot;python\u0026quot;, \u0026quot;-m\u0026quot;, \u0026quot;awslambdaric\u0026quot; ] CMD [ \u0026quot;app.module.handler\u0026quot; ] Assuming that we have a file structure with python package \u0026ldquo;app\u0026rdquo; and a module \u0026ldquo;module\u0026rdquo; which provides the handler function (more on this soon!)\nfastapi_lambda_app \u0026gt; app \u0026gt; module.py module.py:\ndef handler(): return \u0026quot;Lambada Oh\u0026quot; All set, we can now upload our Docker image to the Amazon Elastic Container Registry for use in a Lambda Function.\nHowever, this docker image won\u0026rsquo;t know what to do when Lambda sends it\u0026rsquo;s (event, context) information, not yet.\nMangum, the Python ASGI Lambda hero To deploy FastAPI, or any ASGI compliant Python code via AWS Lambda, all that is required is to wrapp the ASGI application with [Mangum](https://mangum. io)\nRevised module.py:\nimport fastapi from mangum import Mangum app = fastapi.FastAPI(exception_handlers=home.exception_handlers) ... FastAPI application code and paths... ... handler = Mangum(app) Notice, there is no call to uvicorn or any other ASGI gateway library since we just want a function that Lambda can call.\nBuild Docker image and upload to ECR Once properly built and tested locally, we can upload the Lambda-ready image to ECR. This requires an existing ECR repository and appropriate AWS permissions.\ndocker build -t my_app . docker tag my_app:latest \u0026lt;aws_account\u0026gt;.dkr.ecr.us-east-1.amazonaws.com/my_app:latest docker push \u0026lt;aws_account\u0026gt;.dkr.ecr.us-east-1.amazonaws.com/my_app:latest AWS Lambda Function Link A little known feature of AWS Lambda Functions is that they can have their own URL, without the need for a Load Balancer or API Gateway to route requests into the function. This is very useful for simple website apps such as a FastAPI developer site, or any endpoint that doesn\u0026rsquo;t require heavy lifting and processing of requests.\nThe Lambda Function link can be protected by IAM, or open to the public. This URL can then be used as the CNAM in your preferred domain management console, to route actual human users in!\nPart II: Setup CodeDeploy for AWS Lambda automation? As mentioned in part I, we might end up using CodePipeline or another service to automate our FastAPI containerized app deployments to AWS Lambda function. However, here is the setup for CodeDeploy in case it\u0026rsquo;s needed.\nDeploy User for automation and DevOps responsible for CI/CD Create an IAM group and deploy user account with the following roles and without AWS Console login access. So this user should only have an Access Key and Access Secret, but no Password. Additionally, if using SSH for CodeCommit create an ssh key for user with:\nssh-keygen\nThen, upload the contents of public key file ending with \u0026lt;key_name\u0026gt;.pub to the deploy user SSH public keys. Reference:\nsetting-up-ssh-unixes.html\nDeploy User roles and permissions In a production scenario, the Deploy User should only have required Read access to Repos, S3, etc. For testing, allowing full access speeds up development but should be restricted later for security.\nAWSCodeCommitFullAccess AmazonS3FullAccess AWSCodeDeployFullAccess AWSCodeDeployRoleForLambda AWSLambdaRole This IAM user will be used to manage code repositories, create CodeDeploy objects such as Application, Deployment-Groups and Deployments.\nCodeDeploy Service Role To run the actual Lambda functions and put the application into production, AWS Code Deploy requires a separate service-associated account. Create a CodeDeployServiceRole which the AWS service will use to run. This will be associated to the deployment-group upon creation.\nIAM Pass Role Policy for Deploy User Create the special IAM policy and assign it to the deploy user account to create the Deployment Group and \u0026ldquo;pass\u0026rdquo; the CodeDeploy service account permission to execute on it\u0026rsquo;s behalf.\nCreate Policy to allow deploy user IAM-PassRole\naws command to create deployment-group aws deploy create-deployment-group --application-name \u0026lt;name\u0026gt; --deployment-group-name \u0026lt;deployments_name\u0026gt;\u0026gt; \\ --service-role-arn arn:aws:iam::\u0026lt;AWS account\u0026gt;:role/CodeDeployServiceRole { \u0026quot;deploymentGroupId\u0026quot;: \u0026quot;\u0026lt;id\u0026gt;\u0026quot; } ","permalink":"https://ericarellano.tech/posts/aws_codedeploy/","summary":"\u003ch1 id=\"part-1-deploying-fastapi-app-to-aws-lambda-with-docker-image\"\u003ePart 1: Deploying FastAPI app to AWS Lambda with Docker image\u003c/h1\u003e\n\u003cp\u003e\u003cimg alt=\"Create AWS Lambda Function from Docker Image\" loading=\"lazy\" src=\"/images/aws_create_lambda.png\"\u003e\u003c/p\u003e\n\u003cp\u003eThis will show how to build a Docker container that can be run by AWS Lambda from the Elastic Container Registry.\u003cbr\u003e\nThe steps to follow on AWS are currently manual - via the AWS Console and the ECR provided Docker commands to upload\nimages.\u003c/p\u003e\n\u003cp\u003eIn future release, we will automate this release cycle using CodeDeploy, or possibly another AWS service if that turns\nout to be a better fit (CodePipeline? CloudFormation?)\u003c/p\u003e","title":"Deploying FastAPI Docker container to AWS Lambda"},{"content":"The logfile, old frenemy From time to time, shit happens. In this trying time, you will have no choice but to review some logs. Times have changed and whereas our tools of choice used to be egrep, Notepad++ or a good old Nagios handler. Now most likely you will be reading logs in a fancy colorful GUI running Grafana, Kibana, Datadog or whathaveyou replacement tool your employer has had the brilliant idea to use instead of industry best practices Take this Revolutionary moment in time:\ntime=\u0026ldquo;1968-05-02 00:00:49.150\u0026rdquo; level=MERDE msg=\u0026quot;[ Paris is on Fire! ] \u0026quot;DELETE https://capitalism from A. Bunch.of.Situationists - 200 200B in 3100 ¬µs\u0026quot; How do we find out how many people revolted against capitalism in Paris, May of 1968? Well, we analyze the logs.\nUsing Regex You might not be able to get away with using Regex for logs, not until you fix them up a bit. What Data Scientists like to call \u0026ldquo;Extract\u0026rdquo; and \u0026ldquo;Transform\u0026rdquo;, or \u0026ldquo;Data preparation\u0026rdquo;\nBut for now, let\u0026rsquo;s assume that the Paris anarchists decided to generate some nice and orderly, mostly uniform logs.\nHere, Pythons regex engine or built-in module re can shine.\nMethod: Build a regular expression to match your desired log classification information, such as the Time, Date, visited URL, HTTP status code, level of mayhem, error count, etc. For example:\nhours = r\u0026rsquo;\\d\\d\\d\\d-\\d\\d-\\d\\d\\s\\d\\d'\nUse Python\u0026rsquo;s find_all, match or other appropriate method to gather all the results you need in one place.\ndatum = open(log_file).readlines() re.findall(hours, datum) Use Python collections.Counter to find the most prevalent hour of revoutionary strife, the most visited URL, or other interesting data points.\nthe_most = Counter(list(\u0026ldquo;find results\u0026rdquo;))\nThe code: from pathlib import Path import sys from collections import defaultdict, Counter import re logs_dir = Path('.') logs_files = logs_dir.iterdir() list_files = [] data_files = [] INFO_re = 0 ERROR_re = 0 hours_re = [] minutes_re = [] for file in logs_files: if file.suffix == \u0026quot;.py\u0026quot;: continue list_files.append(file) total_lines = 0 datum = '' for file in list_files: with open(file, 'r') as f: datum += f.read() hours = re.findall(r'\\d\\d\\d\\d-\\d\\d-\\d\\d\\s\\d\\d', datum) minutes = re.findall(r'\\d\\d\\d\\d-\\d\\d-\\d\\d\\s\\d\\d:\\d\\d', datum) hours_counter = Counter(hours) minutes_counter = Counter(minutes) max_hour = hours_counter.most_common(1) max_minute = minutes_counter.most_common(1) INFO_re += len(re.findall(r'level=info', datum)) ERROR_re += len(re.findall(r'level=error', datum)) URL_re = re.findall(r'https?://[.\\w\\d/]*\\s', datum) u = Counter(URL_re) top_5 = u.most_common(5) lines = datum.split('\\n') total_lines += len(lines) print(f\u0026quot;Top hour: {max_hour}\u0026quot;, f\u0026quot;Top minute: {max_minute}\u0026quot;) print(f\u0026quot;5 most common urls: {top_5}\u0026quot;) print(f\u0026quot;Info lines by regex: {INFO_re}\u0026quot;) print(f\u0026quot;Info lines by regex: {ERROR_re}\u0026quot;) print(f\u0026quot;We have {total_lines} total lines\u0026quot;) Using Pandas Yeah but, that\u0026rsquo;s old school, right? Everyone wants to use Pandas nowadays, and we are onboard. Pandas is amazing, powerful and fast. However, it doesn\u0026rsquo;t magically read all your data in the fields you want, still gotta do the data preparation.\nAt a large scale, we can call this \u0026ldquo;Extract, Transform and Load\u0026rdquo; and send it through some fancy \u0026ldquo;data pipelines\u0026rdquo;, \u0026ldquo;Kafka stream\u0026rdquo;, \u0026ldquo;Hadoops\u0026rdquo; and thus we enter the realm of Data Engineering.\nBut on a small scale, it\u0026rsquo;s just some Pyton file, buffer and string manipulation, wrangling with Dates, and figuring out again which are our special regular expressions to be found and catalogued.\nThe code: import pandas as pd from pathlib import Path import re logs_dir = Path('.') logs_files = logs_dir.iterdir() lines = [] fields = None for file in logs_files: if file.suffix == \u0026quot;.py\u0026quot;: continue with open(file) as f: while ln := f.readline(): if fields is None: spaces = ln.split(\u0026quot; \u0026quot;) fields = len(spaces) spaces = ln.split(\u0026quot; \u0026quot;) current_fields = len(ln.split(\u0026quot; \u0026quot;)) if current_fields != fields: print(f\u0026quot;Non-standard line ignored: {ln}\u0026quot;) continue date = spaces[0].split(\u0026quot;=\u0026quot;)[1].strip('\u0026quot;').strip(\u0026quot;'\u0026quot;) time = spaces[1].strip('\u0026quot;').strip(\u0026quot;'\u0026quot;) datetime = date + \u0026quot; \u0026quot; + time level = spaces[2].split('=')[1].strip(\u0026quot;'\u0026quot;).strip('\u0026quot;') msg = ' '.join(spaces[3:]).split('=')[1].strip(\u0026quot;\\n\u0026quot;).strip('\u0026quot;') if match := re.search(r'(https?://[\\w./]*)\\s', msg): url = match.group(0) else: url = None date_fmt = \u0026quot;%Y-%m-%dT%H:%M:%S.%f%z\u0026quot; lines.append([datetime, level, url, msg]) names = [\u0026quot;datetime\u0026quot;, \u0026quot;level\u0026quot;, \u0026quot;url\u0026quot;, \u0026quot;msg\u0026quot;] df = pd.DataFrame(lines, columns=names) print(df.url.value_counts()) There we go. in the world of Data, we are! ","permalink":"https://ericarellano.tech/posts/python_logs/","summary":"\u003ch3 id=\"the-logfile-old-frenemy--hahahugoshortcode18s0hbhb\"\u003eThe logfile, old frenemy  \u003cimg src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4a9.png?v8\" alt=\":poop:\" class=\"emoji\" style=\"display: inline-block; height: 1em; width: 1em; vertical-align: middle;\" /\u003e\u003c/h3\u003e\n\u003cp\u003eFrom time to time, shit happens. In this trying time, you will have no choice but to review some logs. Times\nhave changed and whereas our tools of choice used to be \u003cem\u003eegrep\u003c/em\u003e, \u003cem\u003eNotepad++\u003c/em\u003e or a good old \u003cem\u003eNagios\u003c/em\u003e handler.  Now most\nlikely you will be reading logs in a fancy colorful GUI running \u003cem\u003eGrafana\u003c/em\u003e, \u003cem\u003eKibana\u003c/em\u003e, \u003cem\u003eDatadog\u003c/em\u003e or whathaveyou\nreplacement\ntool your employer has had the brilliant idea to use instead of industry best practices  \u003cimg src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f604.png?v8\" alt=\":smile:\" class=\"emoji\" style=\"display: inline-block; height: 1em; width: 1em; vertical-align: middle;\" /\u003e\u003c/p\u003e","title":"Analyzing Logs with Python"},{"content":"Useful Bash commands for DevOps work So Bash is no longer the hottest ticket on the market. Happens. I may have seen the actual point where this peaked and let me tell you, that\u0026rsquo;s something. Still, there\u0026rsquo;s some cool stuff to be done with Bash.\nFor example, if you are writing a quick and dirty Terraform configuration, you can get really far with TF templates, HCL interpolation and well placed Bash commands:\n[Terraform Bash template for Jitsi installation](https://github. com/edamsoft-sre/jitsi-azure-terraform/blob/trunk/environments/prod/templates /setup.tpl)\n1. iterm2 or csshX The best terminal software will practically do your job for you. Just install, sit back and cash in. If you don\u0026rsquo;t know what either is, try iterm2 first. If you can find csshX and install it, then all your problems will be solved.\nYou will have new ones, for sure. but the old ones, gone. Using multiple windows efficiently is almost better than actually automating anything. It can be lot\u0026rsquo;s of fun, and also very dangerous.\niterm Shortcuts Ctrl-shift-i - Send output to all windows AND tabs (watch out!) Ctrl-D - Open a window to the right Ctrl-Shift-D - Open a window below Ctrl-Shift-Return - toggle current window full screen (fill iterm2 window) If you need or WANT to have dozens of servers open and send commands to different sub groups, and, and, and\u0026hellip;.\nGet csshX !!! Then, relax and eat some ramen 2. top The Linux top command is pretty well documented.\nI like to type \u0026ldquo;zbx\u0026rdquo; when I start it, and then capital I and 1 to check the number of cpus/cores.\nThis doesn\u0026rsquo;t do much, but it looks nice.\noptions - *x*: enable bold for current sort_by column - *z*: enable color - *Z*: edit colors - *I*: Irix mode - detects single cpu - *b*: toggle solid column/row highlight background - *k*: kill pid - *m*: memory ( I think, check the man page) - *l*: *load* haha but really, just type *uptime* 3. find Find the largest directories or files in a tree\nfind . -type d -print0 | xargs -0 du -s | sort -n | tail -15 | cut -f2 | xargs -I{} du -sh {} Xargs allows us to pass multiple arguments to a command like Python map, so the disk usage du command receives each subsequent file result from find and gets its size. Then we do some table transformation, basically, and re-run the results through xargs and du to generate a final listing that is sorted across all results.\nRun a command on the findings Another way to find files or directories and execute on command with them is the explicit -exec switch.\nfind . -mtime 0 -exec stat '{}' \\; 4. capture whole packets tcpdump -n -A -q -w /tmp/packet.cap \u0026lt;some filter\u0026gt; Capturing packets is a lot of fun, if you\u0026rsquo;re an evil hacker. Otherwise it\u0026rsquo;s a huge pain in the ass. Most likely, you kids will be looking at packet statistics aggregated in a nice GUI like Grafana, Kibana, or not at all thanks to advances in the software industry. You\u0026rsquo;re welcome.\nnc -v -u google.org 5060 read captures in CLI\ntcpdump -r | less 5. ssh and scp scp -o 'ProxyJump user@bastion' -i key user@host:/path/to/remote/file /local/path/file You should always be using secure shell with strong RSA keys and a good key management system.\nOther than that, use your ~/.ssh/config file to set some sensible defaults\nHost * UseKeyChain yes AddKeysToAgent yes IdentityFile ~/.ssh/id_rsa_private_keyfile Add ssh keys to the macOS agent, if that\u0026rsquo;s your OS. If not, it won\u0026rsquo;t work. Well, it should but who knows what the Ubuntu or RedHat options are. Who cares?\nssh-add -l ssh-add ~/path/to/my/key ssh-add -l Use a passphrase on your keys. Why not.\n6. Awk and sed and cut Learn them well, you can really go down a rabbithole with these tools. Through probably the late 90\u0026rsquo;s they were best in class. But you still have thousands of websites and production platforms relying on a well placed Awk or Sed, trust me.\nUse sed to replace text with regex\nSed allows simple regular expression matching which is what you\u0026rsquo;re going to have to do to fix your mistakes. All of them.\nsed -i \u0026quot;s/component_secret =.*/component_secret = \\\u0026quot;$JICOFO_SECRET\\\u0026quot;/\u0026quot; $PROSODY_HOST_CONF sed -i \u0026quot;s#some_text/next_one#next_one#g\u0026quot; $INPUT_FILE On Ubuntu Hirsute 21.04 get a list of only IPv4 addresses. Can be done in better ways, probably, but you get the jist.\nip -4 addr | awk '{ print $2 }' | sed 's#/.*##' | sed \u0026quot;s#^[^0-9].*##\u0026quot; | sed '/^$/d' | sed 1d Awk is a full programming language, but we can use it to get text fields or csv columns from text. sed regular expression replace with the \u0026rsquo;s/old/new/g\u0026rsquo; command and global switches Regex \u0026lsquo;[^0-9].*\u0026rsquo; match starts with a non-digit sed Delete the first line, which is localhost, and Delete empty lines Use a pound \u0026ldquo;#\u0026rdquo; separator if you need to match a \u0026ldquo;/\u0026rdquo; forward slash., 7. While and For Loops These are also useful. You can make a list of numbers like {1..100}. This is where you realize Bash really IS a programming language. Imagine that! for i in {1..100}; do echo \u0026quot;Some interesting thing one hundred times with an index: $i\u0026quot;; done content = \u0026quot;content coming soon\u0026quot; for i in More do if [ $RANDOM -le 7000 ] then echo \u0026quot;$i groovy $content\u0026quot; else echo \u0026quot;$i questionable $content\u0026quot; fi done Ultimately, you will not use very much Bash scripting, unless you want to. But you can get really far, accessing Databases and stuff. Thought you should know.\n8. Let us know your favorite commands! read feedback; echo $feedback \u0026gt; /dev/null ","permalink":"https://ericarellano.tech/posts/linux_bash/","summary":"\u003ch2 id=\"useful-bash-commands-for-devops-work----hahahugoshortcode15s0hbhb\"\u003eUseful Bash commands for DevOps work    \u003cimg src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f9f0.png?v8\" alt=\":toolbox:\" class=\"emoji\" style=\"display: inline-block; height: 1em; width: 1em; vertical-align: middle;\" /\u003e\u003c/h2\u003e\n\u003cp\u003eSo Bash is no longer the hottest ticket on the market. Happens. I may have seen the actual point where this peaked\nand let me tell you, that\u0026rsquo;s something. Still, there\u0026rsquo;s some cool stuff to be done with Bash.\u003c/p\u003e\n\u003cp\u003eFor example, if you are writing a quick and dirty Terraform configuration, you can get really far with TF templates,\nHCL interpolation and well placed Bash commands:\u003c/p\u003e","title":"The Devops Rough cli / Bash Guide"},{"content":"Download Github Emojis locally En Emojis de Github para Pelican con Markdown armamos los hermosos √≠conos que se pueden ver por este blog. Bueno, en realidad bajamos el archivo json de Github con el diccionarios\n#!python { _short_name_ : _http_url } y utilizamos los enlaces para armar tags img. Poco a poco hemos mejorado algunas cosas, como regualr el tama√±o de los √≠conos 64x64 con CSS.\nPara descargar los 1793 emojis de GitHub la operaci√≥n toma mucho tiempo para tan solo 9.1M: como 15 minutos En realidad es solamente esperar el round trip time entre nosotros y github.com 1800 veces, lo cual es una aplicaci√≥n perfecta para IO mediante Async.\nPodr√≠amos utilizar threading.Thread pero la idea es mirar hacia adelante con optimismo y darle con Async.\nA veces, un diff vale mil palabras y esperemos que esta es una de esas veces.\nComo se puede ver, es f√°cil a√±adir un poco de concurrencia async a un sencillo c√≥digo en Python con Async y aiohttp. Ahorramos valiosos minutos, que en una aplicaci√≥n compleja pueden ser importantes.\nThe code The good old Requests method: @staticmethod def load_from_github(): try: resp = requests.get(SOURCE) payload = resp.content data = json.loads((payload.decode('utf-8'))) return GheEmoji(emoji=data) except Exception as e: print(e) Quick and dirty aiohttp client session and write bytes @staticmethod async def get_bytes(url: str) -\u0026gt; bytes: async with aiohttp.ClientSession() as sesh: async with sesh.get(url) as payload: data = await payload.read() return data @staticmethod async def write_bytes(path: str, data: bytes) -\u0026gt; bool: try: with Path(f\u0026quot;{SAVE_PATH}{path}.png\u0026quot;).open('xb') as file_name: file_name.write(data) except FileExistsError as failed: print(failed) return @staticmethod async def fetch_task(tag: str, url: str): file = url.split('/')[-1] content = await GheEmoji.get_bytes(url) await GheEmoji.write_bytes(tag, content) async def download(self): tasks = [] for tag, url in self.getConfig('emoji').items(): tasks.append(self.fetch_task(tag, url)) await asyncio.wait(tasks) Running the test t0 = datetime.now() # single Get, just uses Requests m = GheEmoji.load_from_github() asyncio.run(m.download()) dt = datetime.now() - t0 print(f\u0026quot;This took {dt} to run\u0026quot;) ","permalink":"https://ericarellano.tech/posts/async_requests/","summary":"\u003ch3 id=\"download-github-emojis-locally\"\u003eDownload Github Emojis locally\u003c/h3\u003e\n\u003cp\u003e\u003cimg alt=\"diff\" loading=\"lazy\" src=\"/images/diff.png\"\u003e\u003c/p\u003e\n\u003cp\u003eEn \u003ca href=\"https://ericarellano.tech/posts/pelican_github_emojis/\"\u003eEmojis de Github para Pelican con Markdown\u003c/a\u003e armamos los hermosos √≠conos que\nse pueden ver por este blog. Bueno, en realidad bajamos el archivo json de Github con el diccionarios\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e#!python\n{ _short_name_ : _http_url }\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ey utilizamos los enlaces para armar tags \u003cem\u003eimg\u003c/em\u003e. Poco a poco hemos mejorado algunas cosas, como regualr el tama√±o de\nlos √≠conos 64x64 con CSS.\u003c/p\u003e\n\u003cp\u003ePara descargar los 1793 emojis de GitHub \u003cimg src=\"https://github.githubassets.com/images/icons/emoji/unicode/00a9.png?v8\" alt=\":copyright:\" class=\"emoji\" style=\"display: inline-block; height: 1em; width: 1em; vertical-align: middle;\" /\u003e  la operaci√≥n toma mucho tiempo para tan solo 9.1M: como 15\nminutos \u003cimg src=\"https://github.githubassets.com/images/icons/emoji/unicode/23f1.png?v8\" alt=\":stopwatch:\" class=\"emoji\" style=\"display: inline-block; height: 1em; width: 1em; vertical-align: middle;\" /\u003e\u003c/p\u003e","title":"Descargas as√≠ncronas con Aiohttp | Async file download"},{"content":"Primero, qu√© es software libre? Algunos diremos: el sistema operativo GNU/Linux. Ok, pero, y eso se come?\nResulta que s√≠. Detr√°s del √©xito de las mega-empresas de tecnolog√≠a que han revolucionado las comunicaciones, relaciones sociales y negocios Linux est√° presente casi exclusivamente.\nTodo desde nuestro tel√©fono inteligente Android/iPhone, televisor inteligente, sistema de mensajes instant√°neos, servicio de correo digital, videoconferencia, streaming de pel√≠culas, en el fondo tienen su base en cientos o miles de servidores Linux, corriendo tambi√©n mucho software libre escrito en lenguajes abiertos: Java (*), Python, Ruby, Bash y con APIs tambi√©n generalmente abiertos.\nEntonces, Software Libre no es s√≥lo una buena idea. Es la realidad fundacional sobre la cual se construye el Internet moderno y las tecnolog√≠as como Aprendizaje de M√°quina (ML) y Telecomunicaciones.\necos Agradeciendo y haciendo eco de las iniciativas de varias personas sobre Software Libre y tecnolog√≠a en Bolivia.\nAprovecho de a√±adir mi granito de arena con otras perspectivas.\nConversando con Eliana Quiroz - Soberan√≠a Tecnol√≥gica y Software Libre\nConversando con Rodrigo Garc√≠a\nCTIC Plan de Implementacion Software Libre (discusion)\nSoftware libre es gratis (preguntemos a Amazon, Google, o Red Hat)\nSoftware libre es deficiente versus al propietario (idem)\nNo existe soporte - que tal stackoverflow, google.com, quora ??\nSoftware libre es revolucionario (ver arriba)\nSoftware libre solucionar√° los problemas estructurales del Estado\nDesarrollo √°gil, colaborativo, abierto\nCompartir conocimientos transversalmente con millones de desarrolladores\nRe-utilizar lo que ya fue hecho\nMediante componentes, sistema operativo, librer√≠as, servicios en la nube\u0026hellip;\nControl soberano de sistemas cr√≠ticos\nDefensa contra ataques digitales\nAhorro en costos de licencias\nMenor costo de soporte, mantenimiento y operaci√≥n\nAs√≠ es. En la nube!!\nDurante muchos a√±os Microsoft e IBM eran la principal competencia de GNU/Linux; que reun√≠a tanto al ala \u0026ldquo;radical\u0026rdquo; de Richard Stallman de la Fundaci√≥n Software Libre y la m√°s pragm√°tica de Linus Torvalds; eso cambi√≥ hace mucho pero quiz√°s el sello lo puso la misma IBM al comprar Red Hat el a√±o pasado.\nPor otro lado, luego de muchos intentos fallidos por irrumpir en el mercado de smartphone, tablet y por mantener el de servidores, Microsoft finalmente logra importantes ganancias con su plataforma en la nube Azure y su sistema de video y mensajer√≠a Microsoft Teams.\nDesconozco el funcionamiento interno de Microsoft sin embargo puedo decir sin lugar a duda que utilizan mucho Software Libre. De muestra un bot√≥n: hace d√≠as el creador del lenguaje Python Guido Van Rossum anunci√≥ que sale del retiro para trabajar con Software Libre en\u0026hellip; Microsoft!\nEntre las otras grandes plataformas de video conferencias: Cisco Webex, Jitsi / 8x8, BlueJeans/Verizon, Google Meet, todas utilizan en alguna medida GNU/Linux.\nY dependen de masivas nubes con infraestructura distribuida.\nEl Estado Boliviano debe brindar una \u0026ldquo;nube soberana\u0026rdquo; como servicio base y precondici√≥n para el mayor desarrollo de la infraestructura digital con el objetivo de:\npermitir plataformas de comunicaci√≥n soberanas\npermitir el acceso ciudadano a datos p√∫blicos de forma autom√°tica, instant√°nea y con alta confiabilidad\npermitir a la ciudadan√≠a acceso a servicios p√∫blicos, tr√°mites, notificaciones, y recursos ante la justicia por medio digital\nEsto se debe construir sobre GNU/Linux, pero no como una decisi√≥n ideol√≥gica sino pragrm√°tica en base al estado actual de la industria.\nCon una nube soberana podemos construir plataformas en conjunto con Amazon (AWS), Google (GCP) o Microsoft (Azure) adem√°s de incorporar sistemas dise√±ados por ingenieros Bolivianos para nuestro mercado y para modernizar al Estado.\nCapacitaci√≥n e investigaci√≥n en Universidades y Escuelas\nServicios de Comunicaci√≥n e intercambio de datos para la burocracia estatal\nLaboratorios de Datos, Inteligencia Artificial, Monitoreo Ambiental\nInvestigacion Cientifica para Hidrocarburos, Litio?\nDise√±o y producci√≥n art√≠stica (gr√°fico, m√∫sica, sonido, video)\nGesti√≥n de datos, anal√≠tica, monitoreo, prevenci√≥n de riesgo para E-commerce, negocios en Internet\nOfim√°tica\nLinux tambi√©n est√° presente en el mundo del Internet de las Cosas (IoT). La mayor√≠a de controladores, sensores y dispositivos que se puedan conectar a la red estatal utilizar√≠an GNU/Linux.\nSin embargo, en este espacio existe ciertamente un fuerte componente privativo en dispositivos como iPhone, tel√©fonos Samsung, los firmware y middleware utilizados por dispositivos ser√≠an privativos.\nEvaluar la seguridad digital y soberan√≠a operativa de los sistemas utilizados o que sean adquiridos por el Estado es otra tarea importante que est√° relacionada al Software Libre pero tiene otros par√°metros m√°s all√° del ideol√≥gico/pol√≠tico.\nLos grandes sistemas del Estado (impuestos, elecciones, presupuestos, registro civil, etc.) seguramente tienen d√©cadas de funcionamiento y una mezcla de procesos manuales y documentos f√≠sicos como tambi√©n digitalizados en sistemas privativos de diferentes tipos.\nTiene sentido mover estos a GNU/Linux? Esto brinda soberan√≠a? Un an√°lisis aparte - pendiente.\nLos funcionarios, consultores y administradores de la burocracia estatal deber√≠an aprender a manejar GNU/Linux? Por supuesto, es una gran herramiente para el futuro.\nSin embargo, la migraci√≥n de Ministerios y otras entidades p√∫blicas hacia GNU/Linux en el escritorio del usuario final, previa educaci√≥n y capacitaci√≥n de los mismos, es tarea del Ministerio de Educaci√≥n.\nEsto no necesariamente se debe mezclar con la implementaci√≥n de una Nube Soberana, servicios p√∫blicos o sistemas Estatales en base a GNU/Linux\nPodemos ver que la pregunta no es realmente si debemos utilizar Software Libre en el Estado. La clave est√° en d√≥nde miramos, c√≥mo se gestiona el software, y cuales son los objetivos concretos.\nTomando una conclusi√≥n general, si se quiere obvia, lo que realmente requiere el Estado Boliviano es una Nube Soberana, Educaci√≥n tecnol√≥gica de calidad, capacitaci√≥n puntual y eficiente, y una estrategia exhaustiva de desarrollo tecnol√≥gico-educativo.\nSoberan√≠a Tecnol√≥gica -\u0026gt; Transparencia -\u0026gt; GNU/Linux -\u0026gt; Nube Soberana -\u0026gt; Instituciones independientes del poder pol√≠tico -\u0026gt; Red fundamental de fibra √≥ptica con PIT regionales de alta disponibilidad\nDatos p√∫blicos del Estado -\u0026gt; transparentes, accesibles de forma autom√°tica -\u0026gt; accesibles program√°ticamente mediante APIs, streams -\u0026gt; pipelines de datos para uso p√∫blico y estatal\nServicios del Estado -\u0026gt; accesibles por Internet y la Red Soberana nacional -\u0026gt; f√°ciles de utilizar, estables, robustos -\u0026gt; control soberano por agencias del Estado -\u0026gt; independencia de consultores y empresas extranjeras\n","permalink":"https://ericarellano.tech/posts/software_libre/","summary":"\u003cp\u003ePrimero, qu√© es software libre? Algunos diremos: el sistema operativo GNU/Linux. Ok, pero, y eso se come?\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Faang\" loading=\"lazy\" src=\"/images/faang.png\"\u003e\u003c/p\u003e\n\u003cp\u003eResulta que s√≠.  Detr√°s del √©xito de las mega-empresas de tecnolog√≠a que han revolucionado las comunicaciones, relaciones sociales y negocios Linux est√° presente casi exclusivamente.\u003c/p\u003e\n\u003cp\u003eTodo desde nuestro tel√©fono inteligente Android/iPhone, televisor inteligente, sistema de mensajes instant√°neos, servicio de correo digital, videoconferencia, streaming de pel√≠culas, en el fondo tienen su base en cientos o miles de servidores Linux, corriendo tambi√©n mucho software libre escrito en lenguajes abiertos: Java (*), Python, Ruby, Bash y con APIs tambi√©n generalmente abiertos.\u003c/p\u003e","title":"Software Libre Soberano?"},{"content":"Pelican blog with Python Markdown Pelican es un generador de microblogs est√°ticos en HTML/Python. Permite utilizar Markdown, RTF, y algunos otros formatos de texto para su contenido. La versi√≥n 4.8.0 es muy robusta e incluye scripts para subir el c√≥digo a cualquier proveedor de nube y desplegar cambios, as√≠ como un servidor de desarrollo y un cat√°logo amplio de extensiones y temas.\nArm√© el blog inicialmente seg√∫n [pelican-hosting-on-appengine.html](http://www.craigjperry. com/pelican-hosting-on-appengine.html), pero me demor√© un poco, y ahora en lugar de Google AppEngine lo despliego mediante Netlify. Reto: Github Emojis en Pelican No se pueden utilizar emojis directamente en Pelican (es decir, con Python Markdown), pero existen varios plugins para lo mismo, y varios repositorios de los c√≥digos Unicode en la web. Python tambi√©n maneja Unicode directamente ¬°¬°lo cual es genial!!\n\u0026gt;\u0026gt;\u0026gt; n = \u0026quot;\\N{FIRE}\u0026quot; \u0026gt;\u0026gt;\u0026gt; n 'üî•' \u0026gt;\u0026gt;\u0026gt; u = \u0026quot;\\U0001F637\u0026quot; \u0026gt;\u0026gt;\u0026gt; u 'üò∑' Pero en este caso quiero los emojis de Github especificamente, porque son m√°s bonitos. M√©todo Consegu√≠ los png del API GitHub con requests, y copiaremos a https://bytefish. de/blog/markdown_emoji_extension salvo que tenemos emojis disponibles en el enlace mencionado, en mi opini√≥n mejores que Unicode est√°ndar y las dem√°s ofertas en internet.\nMarkdown Extension Python Markdown incluye la posiblidad de modificar el archivo .md antes y durante el an√°lisis l√©xico, e inclusive despu√©s de generado el HTML. Para esto se utilizan una variedad de Processors y Handlers que mayormente vienen con el m√≥dulo.\nEstos cubren la mayor√≠a de necesidades como ser HTML tags, im√°genes, s√≠mbolos comunes, etc. Primero, Pelican se configura con un archivo Python sencillo que pasa las opciones deseadas. He desarrollado la clase GheEmoji que permite bajar los datos de Github: { shortname: image_url}\nfrom github_emojis import GheEmoji MARKDOWN = { 'extensions' : [GheEmoji.load_from_github()], # ... } Markdown requiere una expresi√≥n regular o regex para encontrar cada shortname por ejemplo\n\\:robot\\: =\u0026gt; Permitiremos +1 y tags con _\nEMOJI_RE = r'(:)((?:[\\+\\-])?[_0-9a-zA-Z]*?):' Inline Processor Siguiendo el manual de Markdown, creamos una clase para extender Markdown.Extension. Inicialmente, usamos ImageInlinePattern para crear nuestro HTML tag img a partir del shortname del emoji Github. Posteriormente con la versi√≥n 3.4.1 actual, se recomienda utilizar InlineProcessor\nPrevious extension with InlinePattern from markdown.extensions import Extension class GheEmoji(Extension): pattern = EmojiInlinePattern(EMOJI_RE, self.getConfig('emoji')) md.inlinePatterns.add('emoji', pattern, '\u0026gt;not_strong') class EmojiInlinePattern(Pattern): def __init__(self, pattern, emoji): super(EmojiInlinePattern, self).__init__(pattern) self.emoji = emoji def handleMatch(self, m): tag = m.group(3) url = self.emoji.get(tag, '') d. Markdown nos brinda un objeto Match d√≥nde el grupo 1 es reservado, el 2 es nuestro primer ``` : ``` y el tag el 3. Latest Python Markdown with InlineProcessor Aqui utilizamos la misma regex, con una l√≥gica algo diferente. InlineProcessor permite m√°s control sobre el resultado y gestionar espacios o car√°cteres especiales. Adem√°s, aprovech√© el XML Etree para a√±adir el CSS class correspondiente que convierte a los emoji en inline_block del tama√±o del texto.\nclass EmojiInlineProcessor(InlineProcessor): def __init__(self, pattern, emoji): super(EmojiInlineProcessor, self).__init__(pattern) self.emoji = emoji def handleMatch(self, m, data): tag = m.group(2) url = self.emoji.get(tag, '') if not url: return None, None, None div = etree.Element(\u0026quot;div\u0026quot;) div.attrib[\u0026quot;class\u0026quot;] = \u0026quot;ghe_emoji\u0026quot; el = etree.SubElement(div, \u0026quot;img\u0026quot;) el.attrib[\u0026quot;class\u0026quot;] = \u0026quot;ghe_emoji\u0026quot; el.set(\u0026quot;src\u0026quot;, url) el.set(\u0026quot;title\u0026quot;, tag) el.set(\u0026quot;alt\u0026quot;, tag) return div, m.start(0), m.end(0) Python Setup setup( name='python_markdown_gh_emoji', version='0.9', packages=find_packages(), py_modules=['python_markdown_gh_emoji'], install_requires=['markdown\u0026gt;=3.0'], python_requires='\u0026gt;3.7', url='https://github.com/edamsoft-sre/github_emojis', license=\u0026quot;OSI Approved :: GNU General Public License v3 or later (GPLv3+)\u0026quot;, author='EDAM', author_email='eric.arellano@hey.com', description='Markdown extension to provide Github emoji (in Pelican)', classifiers=[ \u0026quot;Programming Language :: Python :: 3.8\u0026quot;, \u0026quot;License :: GNU General Public License v3 or later (GPLv3+)\u0026quot;, \u0026quot;Operating System :: OS Independent\u0026quot;] ) C√≥digo completo\nTest/usage\ntxt = \u0026quot;\u0026quot;\u0026quot; line 1 :fight: line 2 line 3 \u0026quot;\u0026quot;\u0026quot; result = markdown.markdown(txt, extensions=[GheEmoji.load_from_github()]) assert result == \u0026quot;\u0026quot;\u0026quot;\u0026lt;p\u0026gt;line 1 line 2 \u0026lt;img alt=\u0026quot;smiley\u0026quot; src=\u0026quot;https://github.githubassets.com/images/icons/emoji/unicode/1f603.png?v8\u0026quot; title=\u0026quot;smiley\u0026quot; /\u0026gt; line 3 \u0026lt;img alt=\u0026quot;metal\u0026quot; src=\u0026quot;https://github.githubassets.com/images/icons/emoji/unicode/1f918.png?v8\u0026quot; title=\u0026quot;metal\u0026quot; /\u0026gt;\u0026lt;/p\u0026gt;\u0026quot;\u0026quot;\u0026quot; # plus_one = \u0026quot;\u0026quot;\u0026quot; # :+1: the plus sign # \u0026quot;\u0026quot;\u0026quot; # # thumbs_up = markdown.markdown(plus_one, extensions=[GheEmoji.load_from_github()]) # print(thumbs_up) Desplegar el blog con m√≥dulo emoji en Netlify. Netlify nos permite ejecutar cualquier comando Linux, que generalmente ser√° alguna herramienta para builds y en este caso es pelican content. La idea es replicar el entorno de desarrollo y correr python3 setup.py install \u0026amp;\u0026amp; pelican content.\nPara desarrollar este \u0026ldquo;feature\u0026rdquo; y no romper el sitio en vivo, creo un despliego mediante git branch, es decir configuramos Netlify para hacer un branch deploy.\nY z√°s, est√° este blog emojiado En CICD con Netlify hablare m√°s sobre estrategias de despliegue.\nReferencias github-emoji-list\nTutorial:-Writing-Extensions-for-Python-Markdown\nPathLib\n","permalink":"https://ericarellano.tech/posts/pelican_github_emojis/","summary":"\u003ch3 id=\"pelican-blog-with-python-markdown\"\u003ePelican blog with Python Markdown\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://docs.getpelican.com\"\u003ePelican\u003c/a\u003e es un generador de microblogs est√°ticos en HTML/Python. Permite utilizar\nMarkdown, RTF, y algunos otros formatos de texto para su contenido. La versi√≥n 4.8.0 es muy robusta e incluye\nscripts para subir el c√≥digo a cualquier proveedor de nube y desplegar cambios, as√≠ como un servidor de desarrollo y\nun cat√°logo amplio de extensiones y temas.\u003c/p\u003e\n\u003ch2 id=\"mediante-netlify\"\u003eArm√© el blog inicialmente seg√∫n  [pelican-hosting-on-appengine.html](\u003ca href=\"http://www.craigjperry\"\u003ehttp://www.craigjperry\u003c/a\u003e.\ncom/pelican-hosting-on-appengine.html), pero me demor√© un poco, y ahora en lugar de Google AppEngine lo despliego\nmediante Netlify.\u003c/h2\u003e\n\u003ch3 id=\"reto-github-emojis-en-pelican\"\u003eReto: Github Emojis en Pelican\u003c/h3\u003e\n\u003cp\u003eNo se pueden utilizar emojis directamente en Pelican (es decir, con Python Markdown), pero existen varios plugins\npara lo mismo, y varios repositorios de los c√≥digos Unicode en la web. Python tambi√©n maneja Unicode directamente\n¬°¬°lo cual es genial!!\u003c/p\u003e","title":"Emojis de Github con Python Markdown"},{"content":"Authors: Eric Arellano\nNetlify $ The fastest way to build the fastest sites. Netlify es la plataforma PaaS/CICD/DNS con integraci√≥n a Git(hub|lab) utilizada para desplegar este sitio web. En resumen:\nConnect your repository Add your build settings Deploy your website Para ejecutar un app completo en Python o un static site puedes utilizar un n√∫mero de librer√≠as. Incluso Pelican tiene facilidades para usar Invoke o Make y gestionar diferentes contextos. Pero para este humilde blog en Pelican, el comando por defecto de Pelican y el build command sencillo de Netlify es perfecto.\nSegu√≠ el manual y simplemente enlazo mi repo con el output en HTML de Pelican.\n$ pelican content As√≠, le dijimos a Netlify que publique el directorio donde Pelican ha generado HTML.\n$ ls -al output/ -rw-r--r-- 1 3523 Jun 30 00:28 anadir-emojis-de-github-al-blog-de-pelican.html -rw-r--r-- 1 2685 Jun 30 00:28 categories.html drwxr-xr-x 7 224 Jun 30 00:30 category -rw-r--r-- 1 9859 Jun 30 00:28 glosario-devops.html -rw-r--r-- 1 6871 Jun 30 00:28 index.html -rw-r--r-- 1 3156 Jun 30 00:28 netlify-en-10-minutos.html En otro articulo configuramos los emojis de Github. Para lograr ejecutar un deploy de prueba en Netlify, uno sin el modulo Emoji y otro con, controlo esto de la siguiente forma en base al setup.py\nif [ -e setup.py ]; then python setup.py install \u0026amp;\u0026amp; pelican content; else pelican content;fi Una vez confirmado, ejecuto siempre el setup.py ya que quiero tener esos Emoji.\nStork Search (Rust) Adicionalmente en la √∫ltima encarnaci√≥n, el Blog utiliza el tema Papyrus, que incluye pelican-search adem√°s de las tablas de contenido. El Pelican Search se basa en un m√≥dulo Rust que es Stork Search y realmente es muy r√°pido al indexar texto, digamos un ElasticSearch localizado o para el edge, buenazo.\nA√±adir Stork Search es un poco complicado por ser otro lenguaje y plataforma consiguiente de desarrollo. Esto se maneja a√±adiendo otro commando al build: build_stork_search.sh el cual simplemente instala el toolchain de Rust y le comanda instalar Stork.\nPrimero intent√© instalar Stork directamente lo cual funciona, pero luego Netlify no permite poner esto en el PATH de ejecuci√≥n y falla sin encontrarlo. Probablemente es una medida de seguridad importante no permitir la ejecuci√≥n de binarios arbitrarios, entonces no le busqu√© m√°s pies al gato y asumo el costo de instalar todo Rust stable en cada deploy por ahora.\nCould not find Stork in $Path\nwget https://files.stork-search.net/releases/v1.5.0/stork-ubuntu-20-04 chmod +x stork-ubuntu-20-04 Site has been deployed. #!/usr/bin/env bash rustup toolchain install stable cargo install stork-search --locked ","permalink":"https://ericarellano.tech/posts/netlify/","summary":"\u003cp\u003eAuthors: Eric Arellano\u003c/p\u003e\n\u003ch3 id=\"netlify\"\u003eNetlify\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003e$ The fastest way to build the fastest sites.\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg alt=\"buildcommand\" loading=\"lazy\" src=\"/images/build_command.png\"\u003e\u003c/p\u003e\n\u003cp\u003eNetlify es la plataforma PaaS/CICD/DNS con integraci√≥n a Git(hub|lab) utilizada para desplegar este sitio web. En\nresumen:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003col\u003e\n\u003cli\u003eConnect your repository\u003c/li\u003e\n\u003cli\u003eAdd your build settings\u003c/li\u003e\n\u003cli\u003eDeploy your website\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/blockquote\u003e\n\u003cp\u003ePara ejecutar un app completo en Python o un static site puedes utilizar un n√∫mero de librer√≠as. Incluso Pelican\ntiene facilidades para usar \u003cem\u003eInvoke\u003c/em\u003e o \u003cem\u003eMake\u003c/em\u003e y gestionar diferentes contextos. Pero para este humilde blog en Pelican,\nel comando por defecto de Pelican y el build command sencillo de Netlify es perfecto.\u003c/p\u003e","title":"Deploy Pelican on Netlify"},{"content":"Los Conceptos IaC Infrastructure as Code\nIaC se refiere a la definici√≥n en c√≥digo de toda la infraestructura de sistemas, desde redes, load balancers, firewalls, servidores, cluster de Kubernetes, bases de datos, servicios, etc. Generalmente cuando decimos IaC hoy en dia nos referimos a Terraform pero igual puede ser Ansible, Cloud Formation, Azure Resource Manager / Bicep, etc.\nConfiguration Management El manejo estructurado y deliberado de la configuraci√≥n de un sistema (GNU Linux o Microsoft). Esto se realiza mediante algo como Ansible, Terraform y Git, Subversion. Es decir, la configuraci√≥n se almacena en un repositorio dedicado a ese prop√≥sito y actualizado regularmente.\nCI/CD Continuous Integration and Continuous Delivery, or Continous Deployment es la integraci√≥n y distribuci√≥n continua de un paquete de software. Abarca desde el git push inicial en Dev, hasta que el usuario puede interactuar o ver los resultados del cambio en c√≥digo.\nEl objetivo central es lograr la agilidad, escala y confiabilidad para desplegar apps o microservicios en la nube altamente disponibles para un te√≥rico p√∫blico de millones y con varios equipos trabajando en diferenes componentes.\nUn breve ejemplo: este sitio web est√° creado con Netlify\nPaaS La Plataforma como un Servicio es un concepto que entra en su segunda d√©cada renovado con IaC/CICD/SDN y hasta IoT. En los remotos 2000, un PaaS significaba Debian, Ubuntu, CentOS o Windows Server configurado en m√°quinas virtuales, con im√°genes preconfiguradas para cada tech stack, y un paquete de scripts.\nEs decir pod√≠as correr C++, Java, Python, PHP, MySQL/Postgres, o Javascript por ejemplo, con alguna forma de crear/modificar un espacio de trabajo nuevo. Esto es un servicio utilizado por desarrolladores, para implementar o probar idea y prototipos.\nIaaS La infraestructura como servicio es el negocio de AWS, Goocle Cloud Platform, Azure, Rackspace, DigitalOcean y varios otros. Dejemos que ellos hagan lo suyo.\nEn realidad, cualquier organizaci√≥n puede tener IaaS, generalmente es equivalente al ingeniero de sistemas que configura los sistemas, servidores, clusters, bases de datos, APIs externos e internos\u0026hellip; pero de forma autom√°tica, altamente disponible, on demand, mediante Internet.\nPor ejemplo crear una organizaci√≥n de Google Cloud Platform, luego creaar una red virtual VPC con varios subnets, firewalls, reglas de access, VPN y Peering a MongoDB o Confluent Kafka o Redis, clusters de Kubernetes y VMs mediante una solicitud como\nPUT https://cloud.new/gcp_org?Kafka=True\u0026amp;Kubernetes=True\u0026amp;Public=False\u0026amp;CPU=100 Y tener la certeza de que la infra backend va a existir al completar la operaci√≥n y recibir un HTTP 200, luego de un tiempo de espera por supuesto.\nDevOps La combinaci√≥n de Desarollo y Operaciones. Por ejemplo, conjugar el Gitflow de ingenieros de software con el testing y QA, el monitoreo y el troubleshooting realizado por equipos de Operaciones, Soporte, Producto.\nGitflow Git flow es la pr√°ctica de gestionar el Unit Testing, Functional Testing o Continuous Integration de un paquete de software a partir de uno o varios repositorios de c√≥digo que autom√°ticamente generan pruebas y estatus a partir de cada Push de cambios. Esto se realiza con Webhooks estilo Github-Gitlab u otros mecanismos, la cosa es que sea autom√°tico, r√°pido y a prueba de balas Generalmente, se utiliza en branches de desarrollo, prueba y staging para luego pasar el c√≥digo verificado a main y ponerlo en producci√≥n mediante Continous Deployment.\nSDN Software Defined Networking se refiera a la definici√≥n de redes, routers, switches y el plano de control de paquetes y bytes en software.\nA diferencia de IaC o la nube donde podemos definir, es cierto, recursos como un Firewall o un Router, en SDN, la configuraci√≥n de routers, switches, FW, balanceo de carga etc. se define en un c√≥digo centralizado y manejado ciertamente con Configuration Management.\nCisco Nexus y otras plataformas similares brindan este servicio para empresas enormes, proveedores de internet (carriers) y otros que antes ten√≠an flotas de miles de routers y switches.\nSe gestionan y optimizan los protocolos como BGP, OSPF, EIGRP, la transmisi√≥n mediante UDP, TCP, etc. y la Calidad de Servicio QoS de forma centralizada\nSite Reliability Engineering (SRE) Site Reliability Engineering es la arquitectura de nube e infraestructura de software para resiliencia, disponibilidad, escalabilidad y observabilidad. Involucra desde las herramientas de desarrollo, el stack de software y componentes, hasta las bases de datos, las redes y VPC en la nube, peering, y el monitoreo de rendimiento mediante logs, trazas y alertas oportunas.\nLas Herramientas Terraform Terraform brinda un lenguaje de configuraci√≥n HCL abstracto encima de las API de proveedores de nube y datacenter comunes como AWS, Azure, GCP, VMWare, DigitalOcean, OpenStack. Hashicorp Configuration Language o HCL es el idioma de Terraform y se parece algo a C en su syntaxis, y a Python en su l√≥gica.\nPor debajo, Terraform utiliza providers para cada nube, y est√° escrito en Go.\nAnsible Ansible es un sistema y lenguaje de configuraci√≥n en YAML que permite actualizar y configurar servidores, dispositivos de red y nubes completas mediante ssh con una configuraci√≥n sencilla.\nPara mayor complejidad se maneja el sistmema de plantillas Jinja.\nEst√° escrito en Python y utiliza Requests.\nChef \u0026#x1f44d; Chef es un sistema de manejo de configuraci√≥n en Ruby, basado en servidores que manejan un entorno (environment) y configuraciones, y agentes que los despliegan.\nPacker \u0026#x1f44d; Packer de Hashicorp es una herramienta para crear im√°genes AMI (AWS), OVF (VMware), GCP, Azure, con el software requerido, o tomar una im√°gen automatizada de un sistema existente. Se puede partir de un sistema vivo o un .iso y ejecutar un provisioner de forma similar a Terraform, que puede ser Ansible por ejemplo.\nFabric Fabric es una librer√≠a completa en Python para configuraci√≥n y automatizaci√≥n remota o local del sistema en base a SSH u otros transportes. Es un rival empedernido de Ansible y las dem√°s cool kids on the block que tiene un lugar importante en algunas empresas.\nJenkins El est√°ndar industrial para la automatizaci√≥n del desarrollo.\nJenkins inici√≥ como un soporte para pipelines de integraci√≥n cont√≠nua en Java, luego paso a continuous delivery.\nAhora corre la mitad del Internet\nTravis CI Integraci√≥n continua, como un servicio. Quiz√°s, la alternativa m√°s conocida a Jenkins.\nKubernetes Kubernetes es el software poderoso para orquestar o gestionar varios hasta miles de workloads de software simult√°neamente, compartiendo recursos como ser VM\u0026rsquo;s en la nube, bases de datos, espacio de red l√≥gica y separando datos y flujos mediante namespaces.\nKubernetes permite desarrollar r√°pidamente en base a microservicios o monolitos con m√∫ltiples instancias, escaladas horizontalmente o verticalmente de forma autom√°tica y cont√≠nua.\nK8s nos permite abstraer la infrastructura de base como ser CPU, Memoria, IO y regular el uso de la misma en base a cuota y observaci√≥n en tiempo real.\nMuchos o la mayor√≠a de grandes servicios en Internet hoy en d√≠a corren sobre Kubernetes.\nNomad Nomad es la antitesis de Kubernetes, es decir es un software para orquestrar microservicios y servicios a gran\nescala en base a un binario de menos de 100MB, contrapuesto a Kubernetes que son como 10 componentes.\nA√∫n as√≠, la idea b√°sica es la misma, YAML para configuraci√≥n, Consul o un service mesh para coordinar servicios y capacidad para distribuir la carga de varios servicios y paquetes en CPU, Memoria e IO compartida de nube.\nVagrant Una herramienta para virtualizar o correr software de nube o servidor localmente para pruebas. Esto es similar a Minikube, Docker Compose, etc. que permiten correr contenedores Docker en VM o laptop.\nJSON Javascript Object Notation. As√≠, feo como suena. Bueno, es cuesti√≥n de gustos, y sobre eso no vamos a escribir los autores. Mentira, JSON es F.E.O. Ev√≠talo. Evita Javascript. La vida es bella, usa Python.\nSi usted trabaja con Javascript y lo disfruta, mucho √©xito. Tuve una √©poca de JS y la recuerdo con cari√±o. Pero simplemente era mucha inversi√≥n en un idioma que no es √≥ptimo en mi humilde y desinformada opini√≥n. Para desarrollo Web, tenemos a Go, Rust, Python, Ruby on Rails, Hotwire, tanta cosa linda. En fin.\njq Pero, si no te queda otra que manejar JSON, puedes utilizar jq para mayor suerte. Jq es un analizador/formateador l√©xico, si utilizo el termino correctamente, para JSON.\nFacilita leer diferentes datos de un JSON grande, como el retorno de la mayor√≠a de comandos aws_cli o gcloud, o az. Siempre podemos solicitar un resultado en Tabla, YAML, o Texto, pero para automatizar JSON y jq es, lamentablemente lo mejor que hay.\nPara aprender m√°s, visita el tutorial pero no intentes memorizar, DevOps no se trata de eso.\nYAML YAML es un markup language, es decir un texto de configuraci√≥n con reglas claras y uso extendido en sistemas como Ansible, Google Cloud, Amazon Web Services, Azure, y muchos otros. En resumen, un documento YAML inicia con tres guiones (\u0026mdash;) o directamente con el c√≥digo.\nEn YAML se pueden definir variables sencillas, datatypes comunes como strings y integers y flotas, adem√°s de estructuras de datos complejas como Diccionarios recursivos, Listas y combinaciones de ambos en niveles arbitrarios de profundidad.\n--- xid: 0 name: un archivo YAML contiene datos que_no_es: \u0026quot;Yaml Ain't a Markup Language\u0026quot; url: \u0026quot;https://yaml.org\u0026quot; una_lista: - Los Beatles - Radiohead - Rolling Stones - Caf√© Tacuba diccionario: calle13: Residente cadillacs: Vicentico muchas_lineas: | \u0026quot;En la Escuela de Computaci√≥n Po√©tica de Nueva York no importan los resultados, sino la experimentaci√≥n. 'Hardware', programaci√≥n y arte son los tres pilares de este centro educativo en el que la poes√≠a se fusiona con el c√≥digo.\u0026quot; linea_larga: \u0026gt; Marx Keynes Friedman esto es todo la misma linea? Cloud CLI, Command Line Interface Para trabjar con una Nube, generalmente podemos utilizar IaC (Terraform, Ansible) o podemos usar su API directamente con nuestro lenguaje favorito. O podemos utilizar sus herramientas CLI, l√≠nea de comando, shell, etc.\nCon diferencias, todos te permitir√°n crear un nuevo recurso, visualizar los recursos existentes, su estado, y cosas m√°s lindas como acceder a las API de machine learning o de data science.\nPor ejemplo, imaginemos una nube gen√©rica con un binario CLI llamada cloud_cli\nKUBE_CREATION_RESULT=$(cloud_cli create kubernetes \u0026quot;Erics Test Cluster\u0026quot; --preemptible=True \\ --private_subnet=True --aws_peering=$AWS_VPC_ID) cloud_cli show kubernetes --id=$(jq '[.[] | .cluster_id) aws2 Para manejar Amazon Web Services es un requisito conocer y dominar a cierto punto aws2 que es el CLI m√°s nuevo hace como 1 a√±o cuando us√© AWS por √∫ltima vez (hasta pr√≥ximo aviso).\naz El comando CLI para manejar Azure, contraparte del Azure Resource Manager visual y el Bicep en YAML, es az\ngcloud Asimismo, para manejar Google Cloud Platform, vamos a utilizar gcloud.\n","permalink":"https://ericarellano.tech/posts/sre_devops_glossary/","summary":"\u003ch2 id=\"los-conceptos\"\u003eLos Conceptos\u003c/h2\u003e\n\u003ch4 id=\"iac\"\u003eIaC\u003c/h4\u003e\n\u003cp\u003e\u003ca href=\"https://docs.microsoft.com/en-us/azure/devops/learn/what-is-infrastructure-as-code\"\u003eInfrastructure as Code\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eIaC se refiere a la definici√≥n en c√≥digo de toda la infraestructura de sistemas, desde redes, load balancers,\nfirewalls, servidores, cluster de Kubernetes, bases de datos, servicios, etc. Generalmente cuando decimos IaC hoy en\ndia nos referimos a Terraform pero igual puede ser Ansible, Cloud Formation, Azure Resource Manager / Bicep, etc.\u003c/p\u003e\n\u003ch4 id=\"configuration-management\"\u003eConfiguration Management\u003c/h4\u003e\n\u003cp\u003eEl manejo estructurado y deliberado de la configuraci√≥n de un sistema (GNU Linux o Microsoft). Esto se realiza\nmediante algo como Ansible, Terraform y Git, Subversion. Es decir, la configuraci√≥n se almacena en un repositorio\ndedicado a ese prop√≥sito y actualizado regularmente.\u003c/p\u003e","title":"DevOps Glossary in Spanish"},{"content":"Tim O\u0026rsquo;Reilly\u0026rsquo;s WTF: the map for next 10 years What\u0026rsquo;s the Future and Why It\u0026rsquo;s Up to Us Ideas fuerza El mapa cambia y el terreno tambi√©n. No podemos usar el anterior mapa para un nuevo dominio.\nLa econom√≠a de la informaci√≥n, escala e inteligencia artificial permiten nuevos dominios inimaginados.\nLa innovaci√≥n no tiene que ser algo nuevo, sino una forma mejor de hacer algo com√∫n (Uber, Airbnb, Tesla, Google Maps\u0026hellip;)\nCompartir informaci√≥n, educaci√≥n abierta y colaborativa. El proceso como un reflejo del producto (sostenibilidad ambiental, accessibilidad, efectos sociales inesperados)\nIngreso universal b√°sico (UBI) como una alternativa a la automatizaci√≥n y reemplazo de la fuerza laboral por m√°quina y AI.\nEnlace www.oreilly.com/tim/wtf-book.html\n","permalink":"https://ericarellano.tech/posts/wtf/","summary":"\u003ch3 id=\"tim-oreillys-wtf-the-map-for-next-10-years\"\u003eTim O\u0026rsquo;Reilly\u0026rsquo;s WTF: the map for next 10 years\u003c/h3\u003e\n\u003ch5 id=\"whats-the-future-and-why-its-up-to-us\"\u003eWhat\u0026rsquo;s the Future and Why It\u0026rsquo;s Up to Us\u003c/h5\u003e\n\u003cp\u003e\u003cimg alt=\"Book\" loading=\"lazy\" src=\"/images/wtf.jpeg\"\u003e\u003c/p\u003e\n\u003ch4 id=\"ideas-fuerza\"\u003eIdeas fuerza\u003c/h4\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eEl mapa cambia y el terreno tambi√©n. No podemos usar el anterior mapa para un nuevo dominio.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eLa econom√≠a de la informaci√≥n, escala e inteligencia artificial permiten nuevos dominios inimaginados.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eLa innovaci√≥n no tiene que ser algo nuevo, sino una forma mejor de hacer algo com√∫n (Uber, Airbnb, Tesla, Google Maps\u0026hellip;)\u003c/p\u003e","title":"WTF"},{"content":"\u0026ldquo;Igual que en el resto del pa√≠s, los vecinos con menos recursos y menor acceso a la educaci√≥n en lugar de conectarse banda ancha cableada usualmente dependen del plan de datos en su smartphone, ning√∫n sustituto, dados los costos y limitaciones a las descargas, para lo que es posible con conexi√≥n cableada.\n\u0026ldquo;Este no es solamente otro informe. Es un estudio pionero y met√≥dico que deliberadamente marcha a trav√©s de un rango de opciones p√∫blicas/privadas para la construcci√≥n de la red y sugiere la separaci√≥n para la planificaci√≥n entre fibra \u0026lsquo;dark\u0026rsquo; y \u0026rsquo;lit\u0026rsquo;.\n\u0026ldquo;Lo m√°s interesante y la versi√≥n m√°s sencilla de sociedad publico-privada sugerida por CTC\u0026hellip;la ciudad emitir√° la franquicia a una empresa privada que construya la red de fibra \u0026lsquo;dark\u0026rsquo; llegando hasta cada hogar y negocio. Luego el municipio licita de forma p√∫blica la operaci√≥n de la fibra \u0026rsquo;lit\u0026rsquo; a empresas que competir√°n y conectar√°n a los usuarios hasta su hogar. La ciudad no entrar√≠a directamente al negocio y no compite con los proveedores existentes. Pero brinda infraestructura b√°sica que cualquier empresa puede usar. La fibra es, o deber√≠a ser un servicio p√∫blico disponible para todos los \u0026lsquo;americanos\u0026rsquo;\u0026rdquo;\nWired - San Francisco Municipal Fiber\nCTC The Potential for Ubiquitous Fiber-To-The-Premises in San Francisco\n","permalink":"https://ericarellano.tech/posts/internet_municipal_en_san_francisco/","summary":"\u003cp\u003e\u0026ldquo;Igual que en el resto del pa√≠s, los vecinos con menos recursos y menor acceso a la educaci√≥n en lugar de conectarse banda ancha cableada usualmente dependen del plan de datos en su smartphone, ning√∫n sustituto, dados los costos y limitaciones a las descargas, para lo que es posible con conexi√≥n cableada.\u003c/p\u003e\n\u003cp\u003e\u0026ldquo;Este no es solamente otro informe. Es un estudio pionero y met√≥dico  que deliberadamente marcha a trav√©s de un rango de opciones p√∫blicas/privadas para la construcci√≥n de la red y sugiere la separaci√≥n para la planificaci√≥n entre fibra \u0026lsquo;dark\u0026rsquo; y \u0026rsquo;lit\u0026rsquo;.\u003c/p\u003e","title":"Internet Municipal en San Francisco, USA"}]