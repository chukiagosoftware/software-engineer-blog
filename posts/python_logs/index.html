<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Analyzing Logs with Python | Go, Python, AI, Devops, AWS, GCP, Cloud Architecture</title><meta name=keywords content=",SRE,DevOps,Data Science"><meta name=description content="Python logs with Regex and Pandas"><meta name=author content="Eric Arellano"><link rel=canonical href=https://blog.ericarellano.tech/posts/python_logs/><link crossorigin=anonymous href=/assets/css/stylesheet.da3211e5ef867bf2b75fd5a6515cfed7195c011e8ab735694e203810a827097b.css integrity="sha256-2jIR5e+Ge/K3X9WmUVz+1xlcAR6KtzVpTiA4EKgnCXs=" rel="preload stylesheet" as=style><link rel=icon href=https://blog.ericarellano.tech/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://blog.ericarellano.tech/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://blog.ericarellano.tech/favicon-32x32.png><link rel=apple-touch-icon href=https://blog.ericarellano.tech/apple-touch-icon.png><link rel=mask-icon href=https://blog.ericarellano.tech/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://blog.ericarellano.tech/posts/python_logs/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><meta property="og:url" content="https://blog.ericarellano.tech/posts/python_logs/"><meta property="og:site_name" content="Go, Python, AI, Devops, AWS, GCP, Cloud Architecture"><meta property="og:title" content="Analyzing Logs with Python"><meta property="og:description" content="Python logs with Regex and Pandas"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-09-18T00:00:00-07:00"><meta property="article:modified_time" content="2022-09-18T00:00:00-07:00"><meta property="article:tag" content="SRE"><meta property="article:tag" content="DevOps"><meta property="article:tag" content="Data Science"><meta name=twitter:card content="summary"><meta name=twitter:title content="Analyzing Logs with Python"><meta name=twitter:description content="Python logs with Regex and Pandas"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"All Posts","item":"https://blog.ericarellano.tech/posts/"},{"@type":"ListItem","position":2,"name":"Analyzing Logs with Python","item":"https://blog.ericarellano.tech/posts/python_logs/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Analyzing Logs with Python","name":"Analyzing Logs with Python","description":"Python logs with Regex and Pandas","keywords":["","SRE","DevOps","Data Science"],"articleBody":"The logfile, old frenemy :poop: From time to time, shit happens. In this trying time, you will have no choice but to review some logs. Times have changed and whereas our tools of choice used to be egrep, Notepad++ or a good old Nagios handler. Now most likely you will be reading logs in a fancy colorful GUI running Grafana, Kibana, Datadog or whathaveyou replacement tool your employer has had the brilliant idea to use instead of industry best practices :smile: Take this Revolutionary moment in time:\ntime=“1968-05-02 00:00:49.150” level=MERDE msg=\"[ Paris is on Fire! ] \"DELETE https://capitalism from A. Bunch.of.Situationists - 200 200B in 3100 µs\" :fire: How do we find out how many people revolted against capitalism in Paris, May of 1968? Well, we analyze the logs.\nUsing Regex You might not be able to get away with using Regex for logs, not until you fix them up a bit. What Data Scientists like to call “Extract” and “Transform”, or “Data preparation”\nBut for now, let’s assume that the Paris anarchists decided to generate some nice and orderly, mostly uniform logs.\nHere, Pythons regex engine or built-in module re can shine.\nMethod: Build a regular expression to match your desired log classification information, such as the Time, Date, visited URL, HTTP status code, level of mayhem, error count, etc. For example:\nhours = r’\\d\\d\\d\\d-\\d\\d-\\d\\d\\s\\d\\d'\nUse Python’s find_all, match or other appropriate method to gather all the results you need in one place.\ndatum = open(log_file).readlines() re.findall(hours, datum) Use Python collections.Counter to find the most prevalent hour of revoutionary strife, the most visited URL, or other interesting data points.\nthe_most = Counter(list(“find results”))\nThe code: from pathlib import Path import sys from collections import defaultdict, Counter import re logs_dir = Path('.') logs_files = logs_dir.iterdir() list_files = [] data_files = [] INFO_re = 0 ERROR_re = 0 hours_re = [] minutes_re = [] for file in logs_files: if file.suffix == \".py\": continue list_files.append(file) total_lines = 0 datum = '' for file in list_files: with open(file, 'r') as f: datum += f.read() hours = re.findall(r'\\d\\d\\d\\d-\\d\\d-\\d\\d\\s\\d\\d', datum) minutes = re.findall(r'\\d\\d\\d\\d-\\d\\d-\\d\\d\\s\\d\\d:\\d\\d', datum) hours_counter = Counter(hours) minutes_counter = Counter(minutes) max_hour = hours_counter.most_common(1) max_minute = minutes_counter.most_common(1) INFO_re += len(re.findall(r'level=info', datum)) ERROR_re += len(re.findall(r'level=error', datum)) URL_re = re.findall(r'https?://[.\\w\\d/]*\\s', datum) u = Counter(URL_re) top_5 = u.most_common(5) lines = datum.split('\\n') total_lines += len(lines) print(f\"Top hour: {max_hour}\", f\"Top minute: {max_minute}\") print(f\"5 most common urls: {top_5}\") print(f\"Info lines by regex: {INFO_re}\") print(f\"Info lines by regex: {ERROR_re}\") print(f\"We have {total_lines} total lines\") Using Pandas :panda_face: Yeah but, that’s old school, right? Everyone wants to use Pandas nowadays, and we are onboard. Pandas is amazing, powerful and fast. However, it doesn’t magically read all your data in the fields you want, still gotta do the data preparation.\nAt a large scale, we can call this “Extract, Transform and Load” and send it through some fancy “data pipelines”, “Kafka stream”, “Hadoops” :basketball: and thus we enter the realm of Data Engineering.\nBut on a small scale, it’s just some Pyton file, buffer and string manipulation, wrangling with Dates, and figuring out again which are our special regular expressions to be found and catalogued.\nThe code: import pandas as pd from pathlib import Path import re logs_dir = Path('.') logs_files = logs_dir.iterdir() lines = [] fields = None for file in logs_files: if file.suffix == \".py\": continue with open(file) as f: while ln := f.readline(): if fields is None: spaces = ln.split(\" \") fields = len(spaces) spaces = ln.split(\" \") current_fields = len(ln.split(\" \")) if current_fields != fields: print(f\"Non-standard line ignored: {ln}\") continue date = spaces[0].split(\"=\")[1].strip('\"').strip(\"'\") time = spaces[1].strip('\"').strip(\"'\") datetime = date + \" \" + time level = spaces[2].split('=')[1].strip(\"'\").strip('\"') msg = ' '.join(spaces[3:]).split('=')[1].strip(\"\\n\").strip('\"') if match := re.search(r'(https?://[\\w./]*)\\s', msg): url = match.group(0) else: url = None date_fmt = \"%Y-%m-%dT%H:%M:%S.%f%z\" lines.append([datetime, level, url, msg]) names = [\"datetime\", \"level\", \"url\", \"msg\"] df = pd.DataFrame(lines, columns=names) print(df.url.value_counts()) There we go. :swimmer: in the world of Data, we are! :chart: ","wordCount":"652","inLanguage":"en","datePublished":"2022-09-18T00:00:00-07:00","dateModified":"2022-09-18T00:00:00-07:00","author":{"@type":"Person","name":"Eric Arellano"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.ericarellano.tech/posts/python_logs/"},"publisher":{"@type":"Organization","name":"Go, Python, AI, Devops, AWS, GCP, Cloud Architecture","logo":{"@type":"ImageObject","url":"https://blog.ericarellano.tech/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://blog.ericarellano.tech/ accesskey=h title="Go, Python, AI, Devops, AWS, GCP, Cloud Architecture (Alt + H)">Go, Python, AI, Devops, AWS, GCP, Cloud Architecture</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://blog.ericarellano.tech/posts title=Posts><span>Posts</span></a></li><li><a href=https://blog.ericarellano.tech/about title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Analyzing Logs with Python</h1><div class=post-description>Python logs with Regex and Pandas</div><div class=post-meta><span title='2022-09-18 00:00:00 -0700 PDT'>September 18, 2022</span>&nbsp;·&nbsp;<span>Eric Arellano</span></div></header><div class=post-content><h3 id=the-logfile-old-frenemy--hahahugoshortcode17s0hbhb>The logfile, old frenemy
:poop:</h3><p>From time to time, shit happens. In this trying time, you will have no choice but to review some logs. Times
have changed and whereas our tools of choice used to be <em>egrep</em>, <em>Notepad++</em> or a good old <em>Nagios</em> handler. Now most
likely you will be reading logs in a fancy colorful GUI running <em>Grafana</em>, <em>Kibana</em>, <em>Datadog</em> or whathaveyou
replacement
tool your employer has had the brilliant idea to use instead of industry best practices
:smile:</p><p>Take this Revolutionary moment in time:</p><hr><h2 id=bunchofsituationists---200-200b-in-3100-µs>time=&ldquo;1968-05-02 00:00:49.150&rdquo; level=MERDE msg="[ Paris is on Fire! ] "DELETE https://capitalism from A.
Bunch.of.Situationists - 200 200B in 3100 µs"</h2><p>:fire:
How do we find out how many people revolted against capitalism in Paris, May of 1968? Well, we analyze the logs.</p><h3 id=using-regex>Using Regex<a hidden class=anchor aria-hidden=true href=#using-regex>#</a></h3><p>You might not be able to get away with using Regex for logs, not until you fix them up a bit. What Data Scientists
like to call &ldquo;Extract&rdquo; and &ldquo;Transform&rdquo;, or &ldquo;Data preparation&rdquo;</p><p>But for now, let&rsquo;s assume that the Paris anarchists decided to generate some nice and orderly, mostly uniform logs.<br>Here, Pythons regex engine or built-in module <em>re</em> can shine.</p><h4 id=method>Method:<a hidden class=anchor aria-hidden=true href=#method>#</a></h4><ol><li><p>Build a regular expression to match your desired log classification information, such as the Time, Date, visited
URL, HTTP status code, level of mayhem, error count, etc. For example:</p><p>hours = r&rsquo;\d\d\d\d-\d\d-\d\d\s\d\d'</p></li><li><p>Use Python&rsquo;s find_all, match or other appropriate method to gather all the results you need in one place.</p><pre><code>datum = open(log_file).readlines()   
re.findall(hours, datum)
</code></pre></li><li><p>Use Python collections.Counter to find the most prevalent hour of revoutionary strife, the most visited URL, or
other interesting data points.</p><p>the_most = Counter(list(&ldquo;find results&rdquo;))</p></li></ol><h4 id=the-code>The code:<a hidden class=anchor aria-hidden=true href=#the-code>#</a></h4><pre><code>from pathlib import Path
import sys
from collections import defaultdict, Counter
import re

logs_dir = Path('.')

logs_files = logs_dir.iterdir()
list_files = []
data_files = []

INFO_re = 0
ERROR_re = 0

hours_re = []
minutes_re = []

for file in logs_files:
    if file.suffix == &quot;.py&quot;:
        continue
    list_files.append(file)

total_lines = 0
datum = ''
for file in list_files:
    with open(file, 'r') as f:
        datum += f.read()

hours = re.findall(r'\d\d\d\d-\d\d-\d\d\s\d\d', datum)
minutes = re.findall(r'\d\d\d\d-\d\d-\d\d\s\d\d:\d\d', datum)

hours_counter = Counter(hours)
minutes_counter = Counter(minutes)
max_hour = hours_counter.most_common(1)
max_minute = minutes_counter.most_common(1)

INFO_re += len(re.findall(r'level=info', datum))
ERROR_re += len(re.findall(r'level=error', datum))

URL_re = re.findall(r'https?://[.\w\d/]*\s', datum)
u = Counter(URL_re)
top_5 = u.most_common(5)

lines = datum.split('\n')
total_lines += len(lines)

print(f&quot;Top hour: {max_hour}&quot;, f&quot;Top minute: {max_minute}&quot;)

print(f&quot;5 most common urls: {top_5}&quot;)
print(f&quot;Info lines by regex: {INFO_re}&quot;)
print(f&quot;Info lines by regex: {ERROR_re}&quot;)
print(f&quot;We have {total_lines} total lines&quot;)
</code></pre><h3 id=using-pandas--hahahugoshortcode17s3hbhb>Using Pandas
:panda_face:</h3><p>Yeah but, that&rsquo;s old school, right? Everyone wants to use Pandas nowadays, and we are onboard. Pandas is
amazing, powerful and fast. However, it doesn&rsquo;t magically read all your data in the fields you want, still gotta do
the data preparation.</p><p>At a large scale, we can call this &ldquo;Extract, Transform and Load&rdquo; and send it through some
fancy &ldquo;data pipelines&rdquo;, &ldquo;Kafka stream&rdquo;, &ldquo;Hadoops&rdquo;
:basketball:
and thus we enter the realm of Data Engineering.</p><p>But on a small scale, it&rsquo;s just some Pyton file, buffer and string manipulation, wrangling with Dates, and figuring out
again which are our special regular expressions to be found and catalogued.</p><h4 id=the-code-1>The code:<a hidden class=anchor aria-hidden=true href=#the-code-1>#</a></h4><pre><code>  import pandas as pd
  from pathlib import Path
  import re
  
  logs_dir = Path('.')
  logs_files = logs_dir.iterdir()
  
  lines = []
  fields = None
  
  for file in logs_files:
      if file.suffix == &quot;.py&quot;:
          continue
      with open(file) as f:
          while ln := f.readline():
              if fields is None:
                  spaces = ln.split(&quot; &quot;)
                  fields = len(spaces)
              spaces = ln.split(&quot; &quot;)
              current_fields = len(ln.split(&quot; &quot;))
              if current_fields != fields:
                  print(f&quot;Non-standard line ignored: {ln}&quot;)
                  continue
              date = spaces[0].split(&quot;=&quot;)[1].strip('&quot;').strip(&quot;'&quot;)
              time = spaces[1].strip('&quot;').strip(&quot;'&quot;)
              datetime = date + &quot; &quot; + time
              level = spaces[2].split('=')[1].strip(&quot;'&quot;).strip('&quot;')
              msg = ' '.join(spaces[3:]).split('=')[1].strip(&quot;\n&quot;).strip('&quot;')
              if match := re.search(r'(https?://[\w./]*)\s', msg):
                  url = match.group(0)
              else:
                  url = None
              date_fmt = &quot;%Y-%m-%dT%H:%M:%S.%f%z&quot;
              lines.append([datetime, level, url, msg])
  
  names = [&quot;datetime&quot;, &quot;level&quot;, &quot;url&quot;, &quot;msg&quot;]
  df = pd.DataFrame(lines, columns=names)
  print(df.url.value_counts())
</code></pre><p>There we go.
:swimmer:
in the world of Data, we are!
:chart:</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://blog.ericarellano.tech/tags/sre/>SRE</a></li><li><a href=https://blog.ericarellano.tech/tags/devops/>DevOps</a></li><li><a href=https://blog.ericarellano.tech/tags/data-science/>Data Science</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://blog.ericarellano.tech/>Go, Python, AI, Devops, AWS, GCP, Cloud Architecture</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script></body></html>